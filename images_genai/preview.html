<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 2 Figures Preview</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 10px;
        }
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            margin-bottom: 40px;
        }
        .figure-container {
            background: white;
            margin: 30px 0;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .figure-header {
            color: #3498db;
            margin-bottom: 15px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .figure-description {
            color: #555;
            margin-bottom: 15px;
            font-size: 14px;
            line-height: 1.6;
        }
        .figure-image {
            text-align: center;
            margin: 20px 0;
        }
        .figure-image img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .figure-meta {
            background: #ecf0f1;
            padding: 10px;
            border-radius: 4px;
            font-size: 12px;
            color: #7f8c8d;
        }
        .toc {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .toc h2 {
            color: #2c3e50;
            margin-top: 0;
        }
        .toc ul {
            list-style: none;
            padding: 0;
        }
        .toc li {
            margin: 8px 0;
        }
        .toc a {
            color: #3498db;
            text-decoration: none;
        }
        .toc a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>ðŸ“Š Chapter 2 Figures Preview</h1>
    <p class="subtitle">The Architecture of Understanding - All 11 Figures</p>
    
    <div class="toc">
        <h2>ðŸ“‘ Table of Contents</h2>
        <ul>
            <li><a href="#fig-2-1">Figure 2.1: Attention Mechanism Visualization</a></li>
            <li><a href="#fig-2-2">Figure 2.2: Multi-Head Attention</a></li>
            <li><a href="#fig-2-3">Figure 2.3: Transformer Layer Stack</a></li>
            <li><a href="#fig-2-4">Figure 2.4: Pre-training Process</a></li>
            <li><a href="#fig-2-5">Figure 2.5: RLHF Process</a></li>
            <li><a href="#fig-2-6">Figure 2.6: Autoregressive Generation</a></li>
            <li><a href="#fig-2-7">Figure 2.7: Model Size Performance Comparison</a></li>
            <li><a href="#fig-2-8">Figure 2.8: Model Size vs Task Complexity Decision Matrix</a></li>
            <li><a href="#fig-2-9">Figure 2.9: Model Family Comparison Matrix</a></li>
            <li><a href="#fig-2-10">Figure 2.10: Enhanced Research Assistant Architecture</a></li>
            <li><a href="#fig-2-11">Figure 2.11: Enhanced Interface with Performance Dashboard</a></li>
        </ul>
    </div>

    <div class="figure-container" id="fig-2-1">
        <h2 class="figure-header">Figure 2.1: Attention Mechanism Visualization</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Demonstrates how the attention mechanism weighs word relationships to understand which words are most relevant to each other.
            <br><br>
            <strong>Key Insight:</strong> Shows the transformer processing "it" in the sentence and correctly identifying "trophy" as the referent through attention weights.
        </div>
        <div class="figure-image">
            <img src="figure_2_1_attention_mechanism.svg" alt="Attention Mechanism">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.1 (The Transformer Revolution)
        </div>
    </div>

    <div class="figure-container" id="fig-2-2">
        <h2 class="figure-header">Figure 2.2: Multi-Head Attention</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Illustrates how different attention heads specialize in different types of linguistic relationships.
            <br><br>
            <strong>Key Insight:</strong> Shows four distinct heads (Syntax, Semantics, References, Position) working simultaneously to build complete understanding.
        </div>
        <div class="figure-image">
            <img src="figure_2_2_multi_head_attention.svg" alt="Multi-Head Attention">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.1 (The Transformer Revolution)
        </div>
    </div>

    <div class="figure-container" id="fig-2-3">
        <h2 class="figure-header">Figure 2.3: Transformer Layer Stack</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Visualizes how understanding becomes progressively more sophisticated through transformer depth.
            <br><br>
            <strong>Key Insight:</strong> Early layers handle basic syntax, middle layers process complex structures, deep layers perform abstract reasoning.
        </div>
        <div class="figure-image">
            <img src="figure_2_3_layer_stack.svg" alt="Layer Stack">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.1 (The Transformer Revolution)
        </div>
    </div>

    <div class="figure-container" id="fig-2-4">
        <h2 class="figure-header">Figure 2.4: Pre-training Process</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Shows the massive-scale pre-training process and what capabilities emerge from it.
            <br><br>
            <strong>Key Insight:</strong> Simple objective (predict next token) on massive data leads to emergent understanding of grammar, facts, reasoning, and more.
        </div>
        <div class="figure-image">
            <img src="figure_2_4_pretraining.svg" alt="Pre-training Process">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.2 (From Random Weights to Intelligence)
        </div>
    </div>

    <div class="figure-container" id="fig-2-5">
        <h2 class="figure-header">Figure 2.5: RLHF Process</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Demonstrates how Reinforcement Learning from Human Feedback aligns models with human values.
            <br><br>
            <strong>Key Insight:</strong> Humans compare responses and indicate preferences; model learns to produce responses that humans find helpful, honest, and harmless.
        </div>
        <div class="figure-image">
            <img src="figure_2_5_rlhf.svg" alt="RLHF Process">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.2 (From Random Weights to Intelligence)
        </div>
    </div>

    <div class="figure-container" id="fig-2-6">
        <h2 class="figure-header">Figure 2.6: Autoregressive Generation</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Shows how responses are built token by token, with each token informed by all previous context.
            <br><br>
            <strong>Key Insight:</strong> What appears instant is actually a sequential process where each word considers everything that came before.
        </div>
        <div class="figure-image">
            <img src="figure_2_6_autoregressive.svg" alt="Autoregressive Generation">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.2 (From Random Weights to Intelligence)
        </div>
    </div>

    <div class="figure-container" id="fig-2-7">
        <h2 class="figure-header">Figure 2.7: Model Size Performance Comparison</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Comprehensive comparison table showing trade-offs between small, medium, and large models.
            <br><br>
            <strong>Key Insight:</strong> Match model size to task complexity - using large models for simple tasks wastes money; using small models for complex tasks produces poor results.
        </div>
        <div class="figure-image">
            <img src="figure_2_7_model_comparison.svg" alt="Model Comparison">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.3 (The Size Question)
        </div>
    </div>

    <div class="figure-container" id="fig-2-8">
        <h2 class="figure-header">Figure 2.8: Model Size vs Task Complexity Decision Matrix</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> 2D decision matrix helping users select appropriate model size for different task complexities.
            <br><br>
            <strong>Key Insight:</strong> Visual guide showing optimal zones (green), acceptable zones (yellow), and problematic zones (red) for model selection.
        </div>
        <div class="figure-image">
            <img src="figure_2_8_decision_matrix.svg" alt="Decision Matrix">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.3 (The Size Question)
        </div>
    </div>

    <div class="figure-container" id="fig-2-9">
        <h2 class="figure-header">Figure 2.9: Model Family Comparison Matrix</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Comprehensive overview of major LLM families (OpenAI, Anthropic, Meta, Google) and their specializations.
            <br><br>
            <strong>Key Insight:</strong> Each family has distinct strengths - GPT for versatility, Claude for analysis, Llama for customization, Gemini for multimodal.
        </div>
        <div class="figure-image">
            <img src="figure_2_9_model_families.svg" alt="Model Families">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.4 (Meeting the Model Families)
        </div>
    </div>

    <div class="figure-container" id="fig-2-10">
        <h2 class="figure-header">Figure 2.10: Enhanced Research Assistant Architecture</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> System architecture diagram showing all components students will build in the hands-on project.
            <br><br>
            <strong>Key Insight:</strong> Four-layer architecture (UI, Intelligence, Optimization, Generation) demonstrating professional AI application design.
        </div>
        <div class="figure-image">
            <img src="figure_2_10_architecture.svg" alt="System Architecture">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.5 (Hands-On Project)
        </div>
    </div>

    <div class="figure-container" id="fig-2-11">
        <h2 class="figure-header">Figure 2.11: Enhanced Interface with Performance Dashboard</h2>
        <div class="figure-description">
            <strong>Purpose:</strong> Mockup of the actual Streamlit application interface students will create.
            <br><br>
            <strong>Key Insight:</strong> Shows real-time performance metrics, model selection, cost tracking, and analytics - bringing all chapter concepts together.
        </div>
        <div class="figure-image">
            <img src="figure_2_11_interface_mockup.svg" alt="Interface Mockup">
        </div>
        <div class="figure-meta">
            <strong>Referenced in:</strong> Section 2.5 (Hands-On Project)
        </div>
    </div>

    <div style="text-align: center; margin-top: 50px; padding: 20px; color: #7f8c8d;">
        <p>ðŸ“š Chapter 2: The Architecture of Understanding</p>
        <p><em>Introduction to Generative AI with Large Language Models</em></p>
        <p>Dr. Moody â€¢ December 2024</p>
    </div>
</body>
</html>
