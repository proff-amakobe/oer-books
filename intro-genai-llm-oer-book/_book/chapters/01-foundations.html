<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Chapter 1: Foundations of Generative AI – Introduction to Generative AI with Large Language Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/02-llms.html" rel="next">
<link href="../acknowledgments.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-9b98f18118eee809be1c051eb5cc78e4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/01-foundations.html"><span class="chapter-title">Chapter 1: Foundations of Generative AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Introduction to Generative AI with Large Language Models</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Introduction to Generative AI with Large Language Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../acknowledgments.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-foundations.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Chapter 1: Foundations of Generative AI</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-llms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 2: The Architecture of Understanding</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-prompt-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Chapter 3: The Art and Science of Prompting</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link" data-scroll-target="#what-youll-learn">What You’ll Learn</a></li>
  <li><a href="#your-practical-project" id="toc-your-practical-project" class="nav-link" data-scroll-target="#your-practical-project">Your Practical Project</a></li>
  <li><a href="#a-note-on-the-journey-ahead" id="toc-a-note-on-the-journey-ahead" class="nav-link" data-scroll-target="#a-note-on-the-journey-ahead">A Note on the Journey Ahead</a></li>
  </ul></li>
  <li><a href="#learning-outcomes" id="toc-learning-outcomes" class="nav-link" data-scroll-target="#learning-outcomes">Learning Outcomes</a></li>
  <li><a href="#key-terminologies-and-concepts" id="toc-key-terminologies-and-concepts" class="nav-link" data-scroll-target="#key-terminologies-and-concepts">Key Terminologies and Concepts</a></li>
  <li><a href="#what-is-generative-ai" id="toc-what-is-generative-ai" class="nav-link" data-scroll-target="#what-is-generative-ai">1.1 What is Generative AI?</a>
  <ul class="collapse">
  <li><a href="#key-distinctions-the-analyst-vs.-the-creator" id="toc-key-distinctions-the-analyst-vs.-the-creator" class="nav-link" data-scroll-target="#key-distinctions-the-analyst-vs.-the-creator">Key Distinctions: The Analyst vs.&nbsp;The Creator</a></li>
  <li><a href="#why-its-revolutionary" id="toc-why-its-revolutionary" class="nav-link" data-scroll-target="#why-its-revolutionary">Why It’s Revolutionary</a></li>
  <li><a href="#the-fundamental-mechanism" id="toc-the-fundamental-mechanism" class="nav-link" data-scroll-target="#the-fundamental-mechanism">The Fundamental Mechanism</a></li>
  </ul></li>
  <li><a href="#the-evolution-of-text-generation" id="toc-the-evolution-of-text-generation" class="nav-link" data-scroll-target="#the-evolution-of-text-generation">1.2 The Evolution of Text Generation</a>
  <ul class="collapse">
  <li><a href="#the-early-dreamers-1950s-1960s" id="toc-the-early-dreamers-1950s-1960s" class="nav-link" data-scroll-target="#the-early-dreamers-1950s-1960s">The Early Dreamers (1950s-1960s)</a></li>
  <li><a href="#the-statistical-revolution-1980s-1990s" id="toc-the-statistical-revolution-1980s-1990s" class="nav-link" data-scroll-target="#the-statistical-revolution-1980s-1990s">The Statistical Revolution (1980s-1990s)</a></li>
  <li><a href="#the-neural-network-era-2000s-2010s" id="toc-the-neural-network-era-2000s-2010s" class="nav-link" data-scroll-target="#the-neural-network-era-2000s-2010s">The Neural Network Era (2000s-2010s)</a></li>
  <li><a href="#the-transformer-revolution-2017-present" id="toc-the-transformer-revolution-2017-present" class="nav-link" data-scroll-target="#the-transformer-revolution-2017-present">The Transformer Revolution (2017-Present)</a></li>
  <li><a href="#capabilities-today" id="toc-capabilities-today" class="nav-link" data-scroll-target="#capabilities-today">Capabilities Today</a></li>
  <li><a href="#connecting-to-your-journey" id="toc-connecting-to-your-journey" class="nav-link" data-scroll-target="#connecting-to-your-journey">Connecting to Your Journey</a></li>
  <li><a href="#looking-forward" id="toc-looking-forward" class="nav-link" data-scroll-target="#looking-forward">Looking Forward</a></li>
  </ul></li>
  <li><a href="#understanding-tokens-and-embeddings" id="toc-understanding-tokens-and-embeddings" class="nav-link" data-scroll-target="#understanding-tokens-and-embeddings">1.3 Understanding Tokens and Embeddings</a>
  <ul class="collapse">
  <li><a href="#tokenization-breaking-language-into-building-blocks" id="toc-tokenization-breaking-language-into-building-blocks" class="nav-link" data-scroll-target="#tokenization-breaking-language-into-building-blocks">Tokenization: Breaking Language into Building Blocks</a></li>
  <li><a href="#embeddings-teaching-ai-the-geography-of-meaning" id="toc-embeddings-teaching-ai-the-geography-of-meaning" class="nav-link" data-scroll-target="#embeddings-teaching-ai-the-geography-of-meaning">Embeddings: Teaching AI the Geography of Meaning</a></li>
  </ul></li>
  <li><a href="#the-transformer-architecture" id="toc-the-transformer-architecture" class="nav-link" data-scroll-target="#the-transformer-architecture">1.4 The Transformer Architecture</a>
  <ul class="collapse">
  <li><a href="#the-attention-revolution-focus-on-what-matters" id="toc-the-attention-revolution-focus-on-what-matters" class="nav-link" data-scroll-target="#the-attention-revolution-focus-on-what-matters">The Attention Revolution: Focus on What Matters</a></li>
  <li><a href="#multiple-attention-heads-different-perspectives" id="toc-multiple-attention-heads-different-perspectives" class="nav-link" data-scroll-target="#multiple-attention-heads-different-perspectives">Multiple Attention Heads: Different Perspectives</a></li>
  <li><a href="#core-architecture-components" id="toc-core-architecture-components" class="nav-link" data-scroll-target="#core-architecture-components">Core Architecture Components</a></li>
  </ul></li>
  <li><a href="#how-large-language-models-work" id="toc-how-large-language-models-work" class="nav-link" data-scroll-target="#how-large-language-models-work">1.5 How Large Language Models Work</a>
  <ul class="collapse">
  <li><a href="#training-phase-learning-from-the-internet" id="toc-training-phase-learning-from-the-internet" class="nav-link" data-scroll-target="#training-phase-learning-from-the-internet">Training Phase: Learning from the Internet</a></li>
  <li><a href="#the-scale-factor" id="toc-the-scale-factor" class="nav-link" data-scroll-target="#the-scale-factor">The Scale Factor</a></li>
  <li><a href="#inference-generating-text-one-token-at-a-time" id="toc-inference-generating-text-one-token-at-a-time" class="nav-link" data-scroll-target="#inference-generating-text-one-token-at-a-time">Inference: Generating Text One Token at a Time</a></li>
  <li><a href="#context-windows-the-models-working-memory" id="toc-context-windows-the-models-working-memory" class="nav-link" data-scroll-target="#context-windows-the-models-working-memory">Context Windows: The Model’s Working Memory</a></li>
  <li><a href="#connecting-to-your-project" id="toc-connecting-to-your-project" class="nav-link" data-scroll-target="#connecting-to-your-project">Connecting to Your Project</a></li>
  </ul></li>
  <li><a href="#core-project-building-an-ai-powered-research-assistant" id="toc-core-project-building-an-ai-powered-research-assistant" class="nav-link" data-scroll-target="#core-project-building-an-ai-powered-research-assistant">Core Project: Building an AI-Powered Research Assistant</a>
  <ul class="collapse">
  <li><a href="#project-overview" id="toc-project-overview" class="nav-link" data-scroll-target="#project-overview">Project Overview</a></li>
  <li><a href="#project-structure" id="toc-project-structure" class="nav-link" data-scroll-target="#project-structure">Project Structure</a></li>
  </ul></li>
  <li><a href="#step-1-setting-up-your-development-environment" id="toc-step-1-setting-up-your-development-environment" class="nav-link" data-scroll-target="#step-1-setting-up-your-development-environment">Step 1: Setting Up Your Development Environment</a>
  <ul class="collapse">
  <li><a href="#understanding-project-organization" id="toc-understanding-project-organization" class="nav-link" data-scroll-target="#understanding-project-organization">1.1 Understanding Project Organization</a></li>
  <li><a href="#creating-your-project-directory" id="toc-creating-your-project-directory" class="nav-link" data-scroll-target="#creating-your-project-directory">1.2 Creating Your Project Directory</a></li>
  <li><a href="#understanding-virtual-environments" id="toc-understanding-virtual-environments" class="nav-link" data-scroll-target="#understanding-virtual-environments">1.3 Understanding Virtual Environments</a></li>
  <li><a href="#installing-dependencies" id="toc-installing-dependencies" class="nav-link" data-scroll-target="#installing-dependencies">1.4 Installing Dependencies</a></li>
  <li><a href="#setting-up-your-api-key" id="toc-setting-up-your-api-key" class="nav-link" data-scroll-target="#setting-up-your-api-key">1.5 Setting Up Your API Key</a></li>
  </ul></li>
  <li><a href="#step-2-building-your-text-generators" id="toc-step-2-building-your-text-generators" class="nav-link" data-scroll-target="#step-2-building-your-text-generators">Step 2: Building Your Text Generators</a>
  <ul class="collapse">
  <li><a href="#the-text-generators-module" id="toc-the-text-generators-module" class="nav-link" data-scroll-target="#the-text-generators-module">2.1 The Text Generators Module</a></li>
  <li><a href="#understanding-the-code" id="toc-understanding-the-code" class="nav-link" data-scroll-target="#understanding-the-code">2.2 Understanding the Code</a></li>
  </ul></li>
  <li><a href="#step-3-creating-the-application-interface" id="toc-step-3-creating-the-application-interface" class="nav-link" data-scroll-target="#step-3-creating-the-application-interface">Step 3: Creating the Application Interface</a>
  <ul class="collapse">
  <li><a href="#the-complete-application" id="toc-the-complete-application" class="nav-link" data-scroll-target="#the-complete-application">3.1 The Complete Application</a></li>
  <li><a href="#interface-components-explained" id="toc-interface-components-explained" class="nav-link" data-scroll-target="#interface-components-explained">3.2 Interface Components Explained</a></li>
  </ul></li>
  <li><a href="#step-4-running-your-research-assistant" id="toc-step-4-running-your-research-assistant" class="nav-link" data-scroll-target="#step-4-running-your-research-assistant">Step 4: Running Your Research Assistant</a>
  <ul class="collapse">
  <li><a href="#final-project-structure-check" id="toc-final-project-structure-check" class="nav-link" data-scroll-target="#final-project-structure-check">Final Project Structure Check</a></li>
  <li><a href="#usage-instructions" id="toc-usage-instructions" class="nav-link" data-scroll-target="#usage-instructions">Usage Instructions</a></li>
  </ul></li>
  <li><a href="#key-learning-outcomes" id="toc-key-learning-outcomes" class="nav-link" data-scroll-target="#key-learning-outcomes">Key Learning Outcomes</a>
  <ul class="collapse">
  <li><a href="#technical-skills-developed" id="toc-technical-skills-developed" class="nav-link" data-scroll-target="#technical-skills-developed">Technical Skills Developed</a></li>
  <li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding">Conceptual Understanding</a></li>
  </ul></li>
  <li><a href="#chapter-summary" id="toc-chapter-summary" class="nav-link" data-scroll-target="#chapter-summary">Chapter Summary</a>
  <ul class="collapse">
  <li><a href="#what-weve-accomplished" id="toc-what-weve-accomplished" class="nav-link" data-scroll-target="#what-weve-accomplished">What We’ve Accomplished</a></li>
  <li><a href="#the-journey-ahead" id="toc-the-journey-ahead" class="nav-link" data-scroll-target="#the-journey-ahead">The Journey Ahead</a></li>
  <li><a href="#reflection-points" id="toc-reflection-points" class="nav-link" data-scroll-target="#reflection-points">Reflection Points</a></li>
  <li><a href="#congratulations" id="toc-congratulations" class="nav-link" data-scroll-target="#congratulations">Congratulations!</a></li>
  <li><a href="#the-bigger-picture" id="toc-the-bigger-picture" class="nav-link" data-scroll-target="#the-bigger-picture">The Bigger Picture</a></li>
  </ul></li>
  <li><a href="#end-of-chapter-interactive-content" id="toc-end-of-chapter-interactive-content" class="nav-link" data-scroll-target="#end-of-chapter-interactive-content">End-of-Chapter Interactive Content</a>
  <ul class="collapse">
  <li><a href="#assignment-chapter-1-core-project-submission" id="toc-assignment-chapter-1-core-project-submission" class="nav-link" data-scroll-target="#assignment-chapter-1-core-project-submission">Assignment: Chapter 1 Core Project Submission</a></li>
  <li><a href="#discussion-forum-chapter-1---foundations-first-insights" id="toc-discussion-forum-chapter-1---foundations-first-insights" class="nav-link" data-scroll-target="#discussion-forum-chapter-1---foundations-first-insights">Discussion Forum: Chapter 1 - Foundations &amp; First Insights</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 1: Foundations of Generative AI</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to your journey into the fascinating world of Generative Artificial Intelligence! If you’ve ever wondered how ChatGPT writes coherent essays, how DALL-E creates stunning artwork from text descriptions, or how GitHub Copilot suggests code completions, you’re about to discover the foundational principles that make these remarkable capabilities possible.</p>
<p>This opening chapter lays the groundwork for understanding how machines can create human-like text, images, and other content that often seems indistinguishable from human-generated work. We’ll explore the remarkable evolution from simple rule-based systems of the 1960s to today’s powerful Large Language Models (LLMs) that can write essays, answer complex questions, engage in creative storytelling, and even help with programming tasks.</p>
<p>But this isn’t just a theoretical exploration. Throughout this chapter, you’ll not only learn the fundamental concepts but also get your hands dirty by building the foundation of an AI-powered research assistant. This practical component will help solidify your understanding as you see these concepts come to life in working code.</p>
<section id="what-youll-learn" class="level3">
<h3 class="anchored" data-anchor-id="what-youll-learn">What You’ll Learn</h3>
<p>By the end of this chapter, you will:</p>
<ul>
<li>Understand the core principles that enable machines to generate human-like content</li>
<li>Trace the historical evolution from early AI systems to modern generative models</li>
<li>Grasp the key concepts of neural networks, transformers, and attention mechanisms</li>
<li>Distinguish between different types of generative AI approaches and their use cases</li>
<li>Build a working prototype research assistant that demonstrates multiple AI techniques</li>
<li>Evaluate the strengths, limitations, and ethical considerations of generative AI systems</li>
</ul>
</section>
<section id="your-practical-project" class="level3">
<h3 class="anchored" data-anchor-id="your-practical-project">Your Practical Project</h3>
<p>As we progress through the theoretical concepts, you’ll simultaneously develop an AI-powered research assistant that can:</p>
<ul>
<li>Accept user queries in natural language</li>
<li>Generate responses using different AI approaches (rule-based, retrieval-based, and generative)</li>
<li>Demonstrate the evolution of AI capabilities we’ll discuss</li>
<li>Serve as a foundation for more advanced projects in subsequent chapters</li>
</ul>
<p>This hands-on approach ensures that abstract concepts become concrete understanding, preparing you not just to use generative AI tools, but to build and customize them for your own needs.</p>
</section>
<section id="a-note-on-the-journey-ahead" class="level3">
<h3 class="anchored" data-anchor-id="a-note-on-the-journey-ahead">A Note on the Journey Ahead</h3>
<p>Generative AI represents one of the most exciting frontiers in computer science today. While the underlying mathematics can be complex, our approach will be to build intuition first, then gradually introduce the technical details. Don’t worry if some concepts seem challenging at first, each chapter builds carefully on the previous one. By the end of this book, you’ll have both the theoretical knowledge and practical skills to work confidently with large language models.</p>
<p>Let’s begin this adventure together!</p>
<hr>
</section>
</section>
<section id="learning-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="learning-outcomes">Learning Outcomes</h2>
<p>By the end of this chapter, you will be able to:</p>
<ol type="1">
<li><strong>Define</strong> generative AI and explain how it differs from traditional AI approaches</li>
<li><strong>Identify</strong> the key components and architecture of generative systems</li>
<li><strong>Understand</strong> the evolution from rule-based systems to neural networks to transformers</li>
<li><strong>Explain</strong> fundamental concepts like tokens, embeddings, and probability distributions</li>
<li><strong>Set up</strong> a complete development environment for generative AI projects</li>
<li><strong>Build</strong> a simple text generator using multiple approaches (Markov chains and API-based)</li>
<li><strong>Compare</strong> the outputs and limitations of different generative approaches</li>
<li><strong>Implement</strong> the foundational architecture for an AI research assistant</li>
</ol>
<hr>
</section>
<section id="key-terminologies-and-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-terminologies-and-concepts">Key Terminologies and Concepts</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Term</strong></th>
<th><strong>Definition</strong></th>
<th><strong>Example/Context</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Generative AI</strong></td>
<td>AI systems that create new content (text, images, code) rather than just classifying or predicting existing data</td>
<td>ChatGPT writing an essay, DALL-E creating images</td>
</tr>
<tr class="even">
<td><strong>Large Language Model (LLM)</strong></td>
<td>Neural networks trained on vast amounts of text data to understand and generate human-like language</td>
<td>GPT-4, Claude, Llama 2</td>
</tr>
<tr class="odd">
<td><strong>Token</strong></td>
<td>The smallest unit of text that a model processes, often words or parts of words</td>
<td>“Hello” = 1 token, “ChatGPT” = 2 tokens</td>
</tr>
<tr class="even">
<td><strong>Embedding</strong></td>
<td>A numerical representation of text that captures semantic meaning in high-dimensional space</td>
<td>Converting “dog” to a vector like [0.2, -0.1, 0.8, …]</td>
</tr>
<tr class="odd">
<td><strong>Transformer</strong></td>
<td>A neural network architecture that uses attention mechanisms to process sequential data</td>
<td>The “T” in GPT (Generative Pre-trained Transformer)</td>
</tr>
<tr class="even">
<td><strong>Attention Mechanism</strong></td>
<td>A technique that allows models to focus on relevant parts of input when generating output</td>
<td>Focusing on “Paris” when answering “What is the capital of France?”</td>
</tr>
<tr class="odd">
<td><strong>Pre-training</strong></td>
<td>The initial phase where models learn language patterns from massive text datasets</td>
<td>Training GPT on books, articles, and web content</td>
</tr>
<tr class="even">
<td><strong>Fine-tuning</strong></td>
<td>Adapting a pre-trained model for specific tasks or domains</td>
<td>Training a medical AI assistant using healthcare data</td>
</tr>
<tr class="odd">
<td><strong>Inference</strong></td>
<td>The process of using a trained model to generate new outputs</td>
<td>Asking ChatGPT a question and receiving an answer</td>
</tr>
<tr class="even">
<td><strong>Prompt</strong></td>
<td>The input text given to a generative model to elicit a specific response</td>
<td>“Write a professional email about…”</td>
</tr>
<tr class="odd">
<td><strong>Temperature</strong></td>
<td>A parameter controlling randomness in model outputs (0 = deterministic, 1+ = creative)</td>
<td>Low temperature for factual answers, high for creative writing</td>
</tr>
<tr class="even">
<td><strong>API (Application Programming Interface)</strong></td>
<td>A way for different software applications to communicate and share functionality</td>
<td>Using OpenAI’s API to access GPT models in your app</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="what-is-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="what-is-generative-ai">1.1 What is Generative AI?</h2>
<p>Imagine having a conversation with a computer that doesn’t just understand what you’re saying, but can respond with original thoughts, create stories you’ve never heard, or even write code to solve problems you describe in plain English. This isn’t science fiction; it’s the reality of Generative Artificial Intelligence.</p>
<p>Generative Artificial Intelligence (GenAI) is a subset of AI that focuses on creating new, original content, such as text, images, music, videos, or even code, rather than merely analyzing or classifying existing data. Think of it as the difference between a critic who can tell you whether a painting is a Picasso or a Monet, versus an artist who can create an entirely new painting in either style.</p>
<p>Unlike traditional AI systems that operate on discriminative models (like spam detection or sentiment analysis), generative AI leverages sophisticated probabilistic models to synthesize novel outputs that capture and extend the patterns found in their training data.</p>
<section id="key-distinctions-the-analyst-vs.-the-creator" class="level3">
<h3 class="anchored" data-anchor-id="key-distinctions-the-analyst-vs.-the-creator">Key Distinctions: The Analyst vs.&nbsp;The Creator</h3>
<p>To understand what makes generative AI special, let’s compare it with traditional AI approaches:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img7.svg" class="img-fluid figure-img"></p>
<figcaption>Discriminative AI vs Generative AI - Fundamental Approaches to Artificial Intelligence</figcaption>
</figure>
</div>
<p><em>Figure 1.1: Discriminative AI vs Generative AI - Fundamental Approaches to Artificial Intelligence</em></p>
<p>To understand what makes generative AI truly revolutionary, we need to appreciate the fundamental difference between the two approaches to artificial intelligence.</p>
<p><strong>Discriminative AI</strong> represents the traditional approach that most people think of when they hear “artificial intelligence.” These systems excel at classification and prediction tasks, they analyze input data and categorize it into predefined groups. A spam filter examines an email and decides “spam” or “not spam.” A sentiment analyzer reads a product review and determines “positive,” “negative,” or “neutral.” A medical diagnostic system looks at symptoms and suggests possible conditions from a known list.</p>
<p>The key characteristic of discriminative AI is that it works within boundaries defined by its training data. It can only choose from options it has seen before. If you show a spam filter a completely new type of message, it can still classify it as spam or not spam, but it cannot create a new category or explain why in novel terms.</p>
<p><strong>Generative AI</strong>, by contrast, creates something new that didn’t exist before. When you ask ChatGPT to “write a poem about artificial intelligence in the style of Shakespeare,” it doesn’t search through a database of pre-written poems to find a match. Instead, it generates an entirely original poem, word by word, that captures Shakespearean language patterns while addressing a topic Shakespeare never wrote about.</p>
<p>Consider this practical distinction:</p>
<ul>
<li><strong>Discriminative task:</strong> “Is this customer review positive or negative?” → <em>classifies existing content</em></li>
<li><strong>Generative task:</strong> “Write a positive review for this product” → <em>creates new content</em></li>
</ul>
</section>
<section id="why-its-revolutionary" class="level3">
<h3 class="anchored" data-anchor-id="why-its-revolutionary">Why It’s Revolutionary</h3>
<p>The revolutionary nature of generative AI lies in its ability to extrapolate beyond its training data in meaningful ways. When Stable Diffusion generates a completely unique image of a “cyberpunk cat wearing a space helmet,” it’s not copying any image from its training set. Instead, it has learned the underlying concepts of “cyberpunk aesthetics,” “cats,” “space helmets,” and “artistic composition,” then combines them into something novel.</p>
<p>This same principle applies to text generation:</p>
<ul>
<li><strong>Text:</strong> Given the prompt “Describe a futuristic city,” a model might generate: <em>“Neon towers stretched into smog-choked skies, their reflections shimmering on rain-slick streets where autonomous drones hummed like mechanical insects.”</em></li>
<li><strong>Code:</strong> Tools like GitHub Copilot can autocomplete a Python function based on a comment (e.g., “# calculate Fibonacci sequence”)</li>
</ul>
<p>The model isn’t retrieving this text from a database; it’s constructing it based on patterns learned from millions of examples of descriptive writing and coding patterns.</p>
</section>
<section id="the-fundamental-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="the-fundamental-mechanism">The Fundamental Mechanism</h3>
<p>At its core, most generative AI for text operates on a deceptively simple principle: <strong>predict the next token</strong>. Given a sequence of words, what word is most likely to come next? This is essentially a very sophisticated version of your phone’s autocomplete feature, but trained on hundreds of billions of words and capable of maintaining coherence across thousands of tokens.</p>
<p>What makes this simple mechanism so powerful is the scale of training and the sophistication of the underlying neural network architecture. When trained on enough diverse text, these models develop what researchers call “emergent capabilities”, abilities that weren’t explicitly programmed but arise from the patterns learned during training. These include:</p>
<ul>
<li>Understanding context and nuance</li>
<li>Following complex instructions</li>
<li>Reasoning through multi-step problems</li>
<li>Adapting writing style to different contexts</li>
</ul>
<hr>
</section>
</section>
<section id="the-evolution-of-text-generation" class="level2">
<h2 class="anchored" data-anchor-id="the-evolution-of-text-generation">1.2 The Evolution of Text Generation</h2>
<p>Understanding where we are requires knowing where we’ve been. The journey from early AI systems to today’s large language models is a fascinating story of incremental breakthroughs, dead ends, and revolutionary insights.</p>
<section id="the-early-dreamers-1950s-1960s" class="level3">
<h3 class="anchored" data-anchor-id="the-early-dreamers-1950s-1960s">The Early Dreamers (1950s-1960s)</h3>
<p>The dream of machines that could understand and generate human language is as old as computing itself. In 1950, Alan Turing proposed his famous “Imitation Game” (now called the Turing Test), asking whether machines could exhibit intelligent behavior indistinguishable from humans in conversation.</p>
<p><strong>ELIZA (1966):</strong> The first program that made people feel like they were talking to an intelligent entity was remarkably simple. Created by Joseph Weizenbaum at MIT, ELIZA used pattern matching and substitution rules to simulate a Rogerian psychotherapist.</p>
<p>If you typed: “I am feeling sad today” ELIZA might respond: “Why do you say you are feeling sad today?”</p>
<p>ELIZA didn’t understand anything, it simply recognized patterns and applied rules. Yet people became emotionally attached to it, revealing an important truth about human psychology that remains relevant today: we readily anthropomorphize systems that respond in human-like ways.</p>
</section>
<section id="the-statistical-revolution-1980s-1990s" class="level3">
<h3 class="anchored" data-anchor-id="the-statistical-revolution-1980s-1990s">The Statistical Revolution (1980s-1990s)</h3>
<p>As computing power grew, researchers began applying statistical methods to language. The key insight was that language has predictable patterns that can be captured mathematically.</p>
<p><strong>N-gram Models</strong> became the workhorse of this era. These models predict the next word based on the previous N-1 words:</p>
<ul>
<li><strong>Bigram (N=2):</strong> Predicts based on the previous word
<ul>
<li>“The cat sat on the ___” → most likely “mat” or “floor”</li>
</ul></li>
<li><strong>Trigram (N=3):</strong> Uses two previous words for more context
<ul>
<li>“The cat sat <strong><em>” → different predictions than just ”sat </em></strong>”</li>
</ul></li>
</ul>
<p><strong>Limitations:</strong> N-gram models struggle with long-range dependencies. In the sentence “The trophy doesn’t fit in the suitcase because it is too big,” understanding that “it” refers to “trophy” requires looking back several words, beyond typical N-gram ranges.</p>
</section>
<section id="the-neural-network-era-2000s-2010s" class="level3">
<h3 class="anchored" data-anchor-id="the-neural-network-era-2000s-2010s">The Neural Network Era (2000s-2010s)</h3>
<p>Neural networks, while invented decades earlier, became practical for language processing in the 2000s with increased computing power and better training techniques.</p>
<p><strong>Recurrent Neural Networks (RNNs)</strong> introduced the concept of “memory”, instead of just looking at immediate neighbors, these systems could remember and use information from earlier in a sequence.</p>
<p><strong>Long Short-Term Memory (LSTM)</strong> networks solved a critical problem: standard RNNs would “forget” information over long sequences. LSTMs introduced sophisticated gates that control what information to remember, forget, and output.</p>
<p>By the mid-2010s, LSTM-based systems achieved impressive results:</p>
<ul>
<li>Google’s Smart Compose could predict the next words in an email</li>
<li>Translation services improved dramatically</li>
<li>Basic conversational AI became possible</li>
</ul>
<p><strong>But there were limitations:</strong> Processing was sequential (one word at a time), making training slow. Long documents still posed challenges, and training required careful tuning.</p>
</section>
<section id="the-transformer-revolution-2017-present" class="level3">
<h3 class="anchored" data-anchor-id="the-transformer-revolution-2017-present">The Transformer Revolution (2017-Present)</h3>
<p>The 2017 paper “Attention Is All You Need” by Vaswani et al.&nbsp;changed everything. The Transformer architecture introduced several revolutionary concepts:</p>
<ul>
<li><strong>Parallel Processing:</strong> Unlike RNNs, Transformers process all words simultaneously</li>
<li><strong>Self-Attention:</strong> Every word can directly attend to every other word, regardless of distance</li>
<li><strong>Scalability:</strong> The architecture scales efficiently with more compute and data</li>
</ul>
<p><strong>GPT-1 (2018):</strong> OpenAI’s first Generative Pre-trained Transformer demonstrated that pre-training on large text corpora, then fine-tuning for specific tasks, could achieve impressive results across multiple benchmarks.</p>
<p><strong>GPT-2 (2019):</strong> A larger model that could generate coherent multi-paragraph text. OpenAI initially withheld the full model due to concerns about misuse, our first hint of the ethical challenges to come.</p>
<p><strong>GPT-3 (2020):</strong> At 175 billion parameters, GPT-3 demonstrated remarkable “few-shot learning”, it could perform tasks it was never explicitly trained on, just by seeing a few examples in the prompt.</p>
<p>Example prompt:</p>
<pre><code>Translate English to French:
sea otter =&gt; loutre de mer
cheese =&gt; fromage
artificial intelligence =&gt;</code></pre>
<p>GPT-3 could correctly complete this with “intelligence artificielle” without ever being explicitly trained as a translator.</p>
<p><strong>Current Generation (2022-Present):</strong></p>
<ul>
<li><strong>GPT-4:</strong> Multimodal capabilities (text and images), improved reasoning</li>
<li><strong>Claude:</strong> Focus on helpfulness, harmlessness, and honesty</li>
<li><strong>Llama:</strong> Open-source models enabling broader research and customization</li>
<li><strong>Gemini:</strong> Google’s multimodal model with web integration</li>
</ul>
</section>
<section id="capabilities-today" class="level3">
<h3 class="anchored" data-anchor-id="capabilities-today">Capabilities Today</h3>
<p>Modern LLMs can:</p>
<ul>
<li><strong>Question answering:</strong> “What causes rainbows?” → <em>generates scientifically accurate explanation</em></li>
<li><strong>Code generation:</strong> “Write a Python function to calculate prime numbers” → <em>generates working code</em></li>
<li><strong>Creative writing:</strong> “Write a haiku about artificial intelligence” → <em>creates original poetry</em></li>
</ul>
<p><strong>ChatGPT and Beyond:</strong> Today’s models can engage in extended conversations, maintain context across thousands of words, write in specific styles, and even help debug their own generated code.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img1.svg" class="img-fluid figure-img"></p>
<figcaption>The Evolution of Text Generation</figcaption>
</figure>
</div>
<p><em>Figure 1.2: The Evolution of Text Generation - From Rule-Based Systems to Modern Transformers</em></p>
</section>
<section id="connecting-to-your-journey" class="level3">
<h3 class="anchored" data-anchor-id="connecting-to-your-journey">Connecting to Your Journey</h3>
<p>As you build your research assistant throughout this chapter, you’ll experience this evolution firsthand:</p>
<ol type="1">
<li><strong>Phase 1:</strong> Rule-based responses (like ELIZA)</li>
<li><strong>Phase 2:</strong> Retrieval-based answers (like statistical methods)</li>
<li><strong>Phase 3:</strong> Generative responses (using modern transformers)</li>
</ol>
<p>This hands-on progression will make the theoretical concepts we’ve just covered much more concrete and help you understand why each advancement was necessary.</p>
</section>
<section id="looking-forward" class="level3">
<h3 class="anchored" data-anchor-id="looking-forward">Looking Forward</h3>
<p>We’ve come from systems that could barely maintain a coherent conversation to AI that can write novels, solve complex problems, and even engage in philosophical discussions. But this is just the beginning. As we’ll explore in the next section, understanding the different types of generative models will help you choose the right approach for different tasks and understand the exciting developments still to come.</p>
<p>The most remarkable part? We’re still in the early days of this revolution.</p>
<hr>
</section>
</section>
<section id="understanding-tokens-and-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="understanding-tokens-and-embeddings">1.3 Understanding Tokens and Embeddings</h2>
<p>Before we dive into how modern AI generates text, we need to understand how it “sees” and “thinks about” language. Just as you might break down a complex recipe into individual steps, AI systems need to break down text into manageable pieces they can work with.</p>
<section id="tokenization-breaking-language-into-building-blocks" class="level3">
<h3 class="anchored" data-anchor-id="tokenization-breaking-language-into-building-blocks">Tokenization: Breaking Language into Building Blocks</h3>
<p>Imagine trying to teach someone a language by showing them individual LEGO blocks versus showing them complete LEGO structures. Tokenization is the process of deciding what size “blocks” to use when feeding text to an AI model.</p>
<section id="word-level-tokenization-the-whole-lego-sets-approach" class="level4">
<h4 class="anchored" data-anchor-id="word-level-tokenization-the-whole-lego-sets-approach">Word-Level Tokenization: The Whole LEGO Sets Approach</h4>
<p>The most intuitive approach treats each word as a single token:</p>
<ul>
<li>“The quick brown fox” → [“The”, “quick”, “brown”, “fox”]</li>
</ul>
<p>This seems natural to humans, but creates problems for AI:</p>
<ul>
<li><strong>Vocabulary explosion:</strong> English has hundreds of thousands of words, and new ones appear constantly (“selfie,” “blockchain,” “unfriend”)</li>
<li><strong>Unknown words:</strong> What happens when the model encounters “supercalifragilisticexpialidocious”?</li>
<li><strong>Memory inefficiency:</strong> Storing every possible word requires enormous vocabulary lists</li>
</ul>
</section>
<section id="subword-level-tokenization-bpe-the-smart-lego-approach" class="level4">
<h4 class="anchored" data-anchor-id="subword-level-tokenization-bpe-the-smart-lego-approach">Subword-Level Tokenization (BPE): The Smart LEGO Approach</h4>
<p>Byte-Pair Encoding (BPE) finds the sweet spot by learning meaningful chunks smaller than words but larger than characters. It’s like having LEGO blocks of different sizes, some individual pieces, some pre-assembled sections.</p>
<p><em>Example breakdown:</em></p>
<ul>
<li>“Unhappiness” → [“un”, “happiness”]</li>
<li>“Unfriendly” → [“un”, “friendly”]</li>
<li>“Preprocessing” → [“pre”, “process”, “ing”]</li>
</ul>
<p><strong>Why this is brilliant:</strong></p>
<ul>
<li>The model learns that “un-” typically means negation</li>
<li>It can handle new words by combining familiar pieces</li>
<li>“Unfathomable” → [“un”, “fathom”, “able”] (even if it’s never seen this exact word)</li>
</ul>
</section>
<section id="character-level-tokenization-the-individual-lego-brick-approach" class="level4">
<h4 class="anchored" data-anchor-id="character-level-tokenization-the-individual-lego-brick-approach">Character-Level Tokenization: The Individual LEGO Brick Approach</h4>
<p>Breaking text down to individual characters:</p>
<ul>
<li>“Hello” → [“H”, “e”, “l”, “l”, “o”]</li>
</ul>
<p>While this eliminates vocabulary issues entirely, it’s like trying to understand a book by looking at one letter at a time, technically possible, but requiring enormous context to make sense of meaning.</p>
</section>
</section>
<section id="embeddings-teaching-ai-the-geography-of-meaning" class="level3">
<h3 class="anchored" data-anchor-id="embeddings-teaching-ai-the-geography-of-meaning">Embeddings: Teaching AI the Geography of Meaning</h3>
<p>Here’s where things get fascinating. Once text is tokenized, each token needs to be converted into numbers that a computer can actually work with. But not just any numbers, these numbers need to capture the <em>meaning</em> and <em>relationships</em> between words.</p>
<section id="the-vector-space-of-language" class="level4">
<h4 class="anchored" data-anchor-id="the-vector-space-of-language">The Vector Space of Language</h4>
<p>Imagine a vast multidimensional space (typically 768 or 1,024 dimensions, though we can only visualize 2 or 3) where every word has a specific location. Words with similar meanings live in the same neighborhood, while different concepts are far apart.</p>
<p><strong>The Famous Example:</strong></p>
<p>Vector(“king”) - Vector(“man”) + Vector(“woman”) ≈ Vector(“queen”)</p>
<p>This isn’t just a mathematical curiosity, it reveals that the model has learned conceptual relationships:</p>
<ul>
<li><strong>Gender relationships:</strong> king/queen, man/woman, actor/actress</li>
<li><strong>Comparative relationships:</strong> good/better/best, big/bigger/biggest</li>
<li><strong>Categorical relationships:</strong> dog/puppy/canine cluster together</li>
</ul>
</section>
<section id="real-world-implications" class="level4">
<h4 class="anchored" data-anchor-id="real-world-implications">Real-World Implications</h4>
<p>When you ask ChatGPT about “canines,” it automatically connects this to dogs, wolves, puppies, and veterinarians, not because it was explicitly programmed with these connections, but because these concepts live close together in its learned embedding space.</p>
<p>In your research assistant project, you’ll see how embeddings allow the system to find relevant information even when your question uses different words than the source material.</p>
<hr>
</section>
</section>
</section>
<section id="the-transformer-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-transformer-architecture">1.4 The Transformer Architecture</h2>
<p>Now that we understand how text becomes numbers, let’s explore the revolutionary architecture that processes these numbers to generate intelligent responses.</p>
<section id="the-attention-revolution-focus-on-what-matters" class="level3">
<h3 class="anchored" data-anchor-id="the-attention-revolution-focus-on-what-matters">The Attention Revolution: Focus on What Matters</h3>
<p>Imagine reading a complex sentence while highlighting the most important words for understanding its meaning. The self-attention mechanism does exactly this, for every word, it determines which other words in the sentence are most relevant for understanding it.</p>
<section id="self-attention-in-action" class="level4">
<h4 class="anchored" data-anchor-id="self-attention-in-action">Self-Attention in Action</h4>
<p>Consider the sentence: “The animal didn’t cross the street because <em>it</em> was tired.”</p>
<p>When processing the word “it,” the attention mechanism:</p>
<ol type="1">
<li><strong>Looks at all other words</strong> in the sentence</li>
<li><strong>Calculates relevance scores:</strong> How important is each word for understanding “it”?
<ul>
<li>“animal”: High relevance (0.8)</li>
<li>“street”: Low relevance (0.1)</li>
<li>“tired”: Medium relevance (0.4)</li>
<li>“didn’t”: Low relevance (0.1)</li>
</ul></li>
<li><strong>Creates a weighted understanding</strong> of “it” based on these scores</li>
</ol>
<p>This is why modern AI can correctly understand that “it” refers to “the animal,” not “the street”, something that tripped up earlier AI systems regularly.</p>
</section>
</section>
<section id="multiple-attention-heads-different-perspectives" class="level3">
<h3 class="anchored" data-anchor-id="multiple-attention-heads-different-perspectives">Multiple Attention Heads: Different Perspectives</h3>
<p>Transformers don’t just use one attention mechanism, they use many (typically 8-16) attention “heads” simultaneously, each focusing on different types of relationships:</p>
<ul>
<li><strong>Head 1:</strong> Might focus on subject-verb relationships</li>
<li><strong>Head 2:</strong> Might track pronoun references</li>
<li><strong>Head 3:</strong> Might identify cause-and-effect connections</li>
<li><strong>Head 4:</strong> Might recognize sentiment patterns</li>
</ul>
</section>
<section id="core-architecture-components" class="level3">
<h3 class="anchored" data-anchor-id="core-architecture-components">Core Architecture Components</h3>
<section id="encoder-decoder-structure-in-some-models" class="level4">
<h4 class="anchored" data-anchor-id="encoder-decoder-structure-in-some-models">1. Encoder-Decoder Structure (in some models)</h4>
<p>Think of this like a translator who first completely understands a sentence in one language (encoder) before producing the translation (decoder):</p>
<ul>
<li><strong>Encoder:</strong> “Je suis heureux” → <em>[deep understanding representation]</em></li>
<li><strong>Decoder:</strong> <em>[deep understanding]</em> → “I am happy”</li>
</ul>
<p><em>Note: GPT models are decoder-only, while BERT is encoder-only. Different architectures excel at different tasks.</em></p>
</section>
<section id="positional-encoding-keeping-track-of-order" class="level4">
<h4 class="anchored" data-anchor-id="positional-encoding-keeping-track-of-order">2. Positional Encoding: Keeping Track of Order</h4>
<p>Since Transformers process all words simultaneously (unlike humans who read left-to-right), they need a way to understand word order. Positional encoding adds a unique “position signature” to each word:</p>
<ul>
<li>“Dog bites man” vs.&nbsp;“Man bites dog” have the same words but very different meanings</li>
<li>Positional encoding ensures the model knows which word came first</li>
</ul>
</section>
<section id="layer-stacking-deep-understanding" class="level4">
<h4 class="anchored" data-anchor-id="layer-stacking-deep-understanding">3. Layer Stacking: Deep Understanding</h4>
<p>Modern Transformers stack many layers (GPT-3 has 96 layers), with each layer building more sophisticated understanding:</p>
<ul>
<li><strong>Layer 1:</strong> Basic grammar and syntax</li>
<li><strong>Layer 20:</strong> Complex relationships and context</li>
<li><strong>Layer 50:</strong> Abstract reasoning and world knowledge</li>
<li><strong>Layer 96:</strong> Sophisticated inference and generation</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img10.svg" class="img-fluid figure-img"></p>
<figcaption>Transformer Architecture - Core Components and Information Flow</figcaption>
</figure>
</div>
<p><em>Figure 1.3: Transformer Architecture - Core Components and Information Flow</em></p>
<hr>
</section>
</section>
</section>
<section id="how-large-language-models-work" class="level2">
<h2 class="anchored" data-anchor-id="how-large-language-models-work">1.5 How Large Language Models Work</h2>
<p>Now let’s put it all together and understand how these components combine to create the AI assistants we interact with today.</p>
<section id="training-phase-learning-from-the-internet" class="level3">
<h3 class="anchored" data-anchor-id="training-phase-learning-from-the-internet">Training Phase: Learning from the Internet</h3>
<section id="the-objective-becoming-a-prediction-master" class="level4">
<h4 class="anchored" data-anchor-id="the-objective-becoming-a-prediction-master">The Objective: Becoming a Prediction Master</h4>
<p>LLMs are trained on a deceptively simple task: <strong>predict the next word</strong>. Given “The sky is ___,” the model learns that “blue,” “gray,” or “cloudy” are more likely than “purple” or “triangular.”</p>
<p>But this simple objective, applied to trillions of words, leads to emergent understanding of:</p>
<ul>
<li><strong>Grammar:</strong> Learning language rules through pattern recognition</li>
<li><strong>Facts:</strong> “The capital of France is ___” → “Paris”</li>
<li><strong>Reasoning:</strong> “If it’s raining, then the ground will be ___” → “wet”</li>
<li><strong>Style:</strong> Formal vs.&nbsp;casual language patterns</li>
<li><strong>Context:</strong> Understanding how meaning changes in different situations</li>
</ul>
</section>
</section>
<section id="the-scale-factor" class="level3">
<h3 class="anchored" data-anchor-id="the-scale-factor">The Scale Factor</h3>
<p>What makes modern LLMs different from earlier attempts isn’t fundamentally new algorithms, it’s scale:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Year</th>
<th>Parameters</th>
<th>Training Data</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-1</td>
<td>2018</td>
<td>117 million</td>
<td>~5GB text</td>
</tr>
<tr class="even">
<td>GPT-2</td>
<td>2019</td>
<td>1.5 billion</td>
<td>~40GB text</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>2020</td>
<td>175 billion</td>
<td>~570GB text</td>
</tr>
<tr class="even">
<td>GPT-4</td>
<td>2023</td>
<td>~1.7 trillion*</td>
<td>Unknown</td>
</tr>
</tbody>
</table>
<p><em>Estimated; OpenAI hasn’t disclosed exact figures</em></p>
</section>
<section id="inference-generating-text-one-token-at-a-time" class="level3">
<h3 class="anchored" data-anchor-id="inference-generating-text-one-token-at-a-time">Inference: Generating Text One Token at a Time</h3>
<p>When you send a prompt to ChatGPT, here’s what happens:</p>
<ol type="1">
<li><strong>Tokenization:</strong> Your text is broken into tokens</li>
<li><strong>Embedding:</strong> Each token becomes a high-dimensional vector</li>
<li><strong>Processing:</strong> The transformer layers process these vectors</li>
<li><strong>Prediction:</strong> The model outputs probabilities for the next token</li>
<li><strong>Selection:</strong> A token is chosen (affected by temperature setting)</li>
<li><strong>Repeat:</strong> Steps 1-5 repeat until the response is complete</li>
</ol>
<section id="the-temperature-dial" class="level4">
<h4 class="anchored" data-anchor-id="the-temperature-dial">The Temperature Dial</h4>
<p>Temperature controls the “creativity” of outputs:</p>
<ul>
<li><strong>Temperature 0:</strong> Always picks the most likely token (deterministic, repetitive)</li>
<li><strong>Temperature 0.7:</strong> Balanced between coherence and creativity (common default)</li>
<li><strong>Temperature 1.0+:</strong> More random, creative, but potentially incoherent</li>
</ul>
<p><strong>Example with the prompt “The best way to learn programming is…”:</strong></p>
<ul>
<li><strong>Temperature 0:</strong> “…to practice regularly and work on projects.”</li>
<li><strong>Temperature 0.7:</strong> “…to dive into real projects that challenge you while building a strong foundation in fundamentals.”</li>
<li><strong>Temperature 1.2:</strong> “…to dance with code like a curious explorer mapping uncharted digital territories.”</li>
</ul>
</section>
</section>
<section id="context-windows-the-models-working-memory" class="level3">
<h3 class="anchored" data-anchor-id="context-windows-the-models-working-memory">Context Windows: The Model’s Working Memory</h3>
<p>Every LLM has a context window, the maximum amount of text it can “see” at once. This includes both your input and the generated output.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Context Window</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>GPT-3.5</td>
<td>4,096 tokens (~3,000 words)</td>
</tr>
<tr class="even">
<td>GPT-4</td>
<td>8,192-128,000 tokens</td>
</tr>
<tr class="odd">
<td>Claude 2</td>
<td>100,000 tokens (~75,000 words)</td>
</tr>
<tr class="even">
<td>Llama 2</td>
<td>4,096 tokens</td>
</tr>
</tbody>
</table>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Longer context = better understanding of complex documents</li>
<li>Longer context = higher computational cost</li>
<li>Beyond the context window, the model literally cannot see earlier content</li>
</ul>
</section>
<section id="connecting-to-your-project" class="level3">
<h3 class="anchored" data-anchor-id="connecting-to-your-project">Connecting to Your Project</h3>
<p>Understanding these mechanisms helps you make better decisions when building AI applications:</p>
<ul>
<li><strong>Tokenization awareness</strong> when designing prompts</li>
<li><strong>Temperature tuning</strong> for different use cases</li>
<li><strong>Context management</strong> for long conversations</li>
<li><strong>Generation strategies</strong> when crafting responses</li>
<li><strong>Hallucination mitigation</strong> when implementing fact-checking features</li>
</ul>
<p>Understanding these fundamentals will help you build more effective AI applications and debug issues when they arise.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img8.png" class="img-fluid figure-img"></p>
<figcaption>Summary of Foundational Concepts</figcaption>
</figure>
</div>
<p><em>Figure 1.4: Summary of Foundational Concepts in Generative AI</em></p>
<hr>
</section>
</section>
<section id="core-project-building-an-ai-powered-research-assistant" class="level2">
<h2 class="anchored" data-anchor-id="core-project-building-an-ai-powered-research-assistant">Core Project: Building an AI-Powered Research Assistant</h2>
<section id="project-overview" class="level3">
<h3 class="anchored" data-anchor-id="project-overview">Project Overview</h3>
<p>Welcome to the beginning of an exciting journey! Over the course of this book, we’ll progressively build a sophisticated AI-powered research assistant. <strong>This chapter focuses on laying the foundation</strong>, creating a simple but extensible system that we’ll enhance with new capabilities in each subsequent chapter.</p>
<p><strong>What We’re Building in Chapter 1:</strong> By the end of this chapter, you’ll have a basic research assistant that can:</p>
<ol type="1">
<li><strong>Accept user research queries</strong> through a clean web interface</li>
<li><strong>Generate responses using Markov chains</strong> (demonstrating statistical text generation from the 1990s)</li>
<li><strong>Compare those responses with modern LLM outputs</strong> (showing the power of current AI)</li>
<li><strong>Provide a solid foundation</strong> for the advanced features we’ll add in later chapters</li>
</ol>
<p>This isn’t our final destination; it’s the launchpad for a system that will eventually include RAG capabilities, multimodal processing, fine-tuned models, and production deployment features.</p>
<p><strong>What You’ll Learn in This Foundation:</strong></p>
<ul>
<li>How to structure a modular, extensible AI application</li>
<li>The practical differences between statistical and neural text generation</li>
<li>How to integrate modern LLM APIs safely and effectively</li>
<li>Best practices for building user-friendly AI interfaces</li>
<li>How to create a codebase that can grow throughout this book</li>
</ul>
</section>
<section id="project-structure" class="level3">
<h3 class="anchored" data-anchor-id="project-structure">Project Structure</h3>
<p>Let’s start with a clean, simple structure that we’ll expand throughout the book:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img5.svg" class="img-fluid figure-img"></p>
<figcaption>Project Structure Diagram</figcaption>
</figure>
</div>
<p><em>Figure 1.5: Project Structure for the AI Research Assistant</em></p>
<p><strong>Note:</strong> This is our starting structure. As we progress through the book, we’ll add:</p>
<ul>
<li><strong>Chapter 2</strong>: Model comparison and selection tools</li>
<li><strong>Chapter 3</strong>: Advanced prompt engineering modules</li>
<li><strong>Chapter 4</strong>: Multi-provider API integration</li>
<li><strong>Chapter 5</strong>: Vector database and RAG components</li>
<li><strong>Chapter 6</strong>: Fine-tuning and model customization tools</li>
<li><strong>…and much more!</strong></li>
</ul>
<hr>
</section>
</section>
<section id="step-1-setting-up-your-development-environment" class="level2">
<h2 class="anchored" data-anchor-id="step-1-setting-up-your-development-environment">Step 1: Setting Up Your Development Environment</h2>
<p>Before we write any code, let’s create a solid development environment. This setup will serve you throughout the book and is similar to what you’d use in professional AI development.</p>
<section id="understanding-project-organization" class="level3">
<h3 class="anchored" data-anchor-id="understanding-project-organization">1.1 Understanding Project Organization</h3>
<p>Professional AI projects need clean organization. Here’s our structure:</p>
<pre><code>research_assistant/
├── app.py              # Main Streamlit application
├── generators.py       # Text generation classes
├── requirements.txt    # Project dependencies
├── .env               # API keys (never commit this!)
└── .gitignore         # Files to exclude from git</code></pre>
</section>
<section id="creating-your-project-directory" class="level3">
<h3 class="anchored" data-anchor-id="creating-your-project-directory">1.2 Creating Your Project Directory</h3>
<p>Open your terminal and navigate to where you want to create your project:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the project directory</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> research_assistant</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> research_assistant</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create empty files for our project</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">touch</span> app.py generators.py requirements.txt .env .gitignore</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="understanding-virtual-environments" class="level3">
<h3 class="anchored" data-anchor-id="understanding-virtual-environments">1.3 Understanding Virtual Environments</h3>
<p><strong>Why Virtual Environments Matter:</strong></p>
<p>Think of virtual environments as isolated containers for your project. Without them:</p>
<ul>
<li>Installing a package for Project A might break Project B</li>
<li>Different projects might need different versions of the same library</li>
<li>Your global Python installation becomes cluttered</li>
</ul>
<p><strong>Creating Your Virtual Environment:</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a virtual environment named 'venv'</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> venv venv</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate it (you'll need to do this each time you work on the project)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># On macOS/Linux:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venv/bin/activate</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># On Windows:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="ex">venv\Scripts\activate</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># You'll see (venv) appear in your terminal prompt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p><strong>💡 Pro Tip:</strong> Always activate your virtual environment before installing packages or running your code. If you see <code>(venv)</code> in your terminal prompt, you’re good to go!</p>
</blockquote>
</section>
<section id="installing-dependencies" class="level3">
<h3 class="anchored" data-anchor-id="installing-dependencies">1.4 Installing Dependencies</h3>
<p>Create your <code>requirements.txt</code> file with these dependencies:</p>
<pre><code># Core dependencies for Chapter 1
streamlit&gt;=1.28.0
openai&gt;=1.0.0
python-dotenv&gt;=1.0.0</code></pre>
<p>Now install them:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Understanding each dependency:</strong></p>
<ul>
<li><code>streamlit</code>: Creates beautiful web interfaces with minimal code</li>
<li><code>openai</code>: Official library for accessing GPT models</li>
<li><code>python-dotenv</code>: Safely loads API keys from environment files</li>
</ul>
</section>
<section id="setting-up-your-api-key" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-your-api-key">1.5 Setting Up Your API Key</h3>
<p><strong>Getting an OpenAI API Key:</strong></p>
<ol type="1">
<li>Visit <a href="https://platform.openai.com/">platform.openai.com</a></li>
<li>Create an account or sign in</li>
<li>Navigate to API Keys section</li>
<li>Click “Create new secret key”</li>
<li><strong>⚠️ Critical:</strong> Copy immediately, you won’t see it again!</li>
</ol>
<p><strong>Storing Your Key Safely:</strong></p>
<p>Add your API key to the <code>.env</code> file:</p>
<pre><code>OPENAI_API_KEY=sk-your-api-key-here</code></pre>
<p><strong>Security Best Practice:</strong> Add <code>.env</code> to your <code>.gitignore</code> file:</p>
<pre><code># .gitignore
.env
venv/
__pycache__/
*.pyc</code></pre>
<blockquote class="blockquote">
<p><strong>⚠️ Warning:</strong> Never commit API keys to version control. A leaked key can result in unexpected charges and security vulnerabilities.</p>
</blockquote>
<hr>
</section>
</section>
<section id="step-2-building-your-text-generators" class="level2">
<h2 class="anchored" data-anchor-id="step-2-building-your-text-generators">Step 2: Building Your Text Generators</h2>
<p>Now comes the exciting part, building the intelligence layer of your research assistant. We’ll create two different approaches to text generation, each representing different eras of AI development.</p>
<section id="the-text-generators-module" class="level3">
<h3 class="anchored" data-anchor-id="the-text-generators-module">2.1 The Text Generators Module</h3>
<p>Create <code>generators.py</code> with the following content:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">Text Generation Module for AI Research Assistant</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">Chapter 1: Foundations of Generative AI</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">This module contains two text generators that demonstrate the evolution</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">of AI text generation:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">1. MarkovChainGenerator: Statistical text generation (1990s approach)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">2. LLMGenerator: Modern large language model generation (2020s approach)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">The comparison between these approaches helps illustrate why transformers</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">revolutionized natural language processing.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Load environment variables from .env file</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MarkovChainGenerator:</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co">    A Markov Chain text generator that learns patterns from training text.</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co">    This represents the statistical approach to text generation that dominated</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co">    the 1990s and early 2000s. It predicts the next word based only on the</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co">    previous N words (the 'order' of the chain).</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Limitations demonstrated:</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co">    - No understanding of meaning or context</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="co">    - Can only reproduce patterns it has seen</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co">    - Struggles with long-range dependencies</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co">    - No ability to follow instructions or answer questions</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co">        order (int): Number of previous words to consider</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="co">        chain (dict): Transition probabilities between word sequences</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co">        starts (list): Valid starting sequences for generation</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, order: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>):</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize the Markov chain.</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a><span class="co">            order: Number of previous words to consider when predicting</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="co">                   the next word. Higher = more coherent but less creative.</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.order <span class="op">=</span> order</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chain <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.starts <span class="op">=</span> []</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _preprocess_text(<span class="va">self</span>, text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">list</span>:</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="co">        Clean and tokenize input text.</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="co">        This preprocessing step is crucial for building a useful model:</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="co">        - Normalizes whitespace</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="co">        - Handles basic punctuation</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a><span class="co">        - Creates a list of tokens (words)</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a><span class="co">            text: Raw input text</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a><span class="co">            List of cleaned tokens</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize whitespace and convert to lowercase</span></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r'</span><span class="dv">\s</span><span class="op">+</span><span class="vs">'</span>, <span class="st">' '</span>, text.strip().lower())</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into words while keeping some punctuation attached</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> text.split()</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> words</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a><span class="co">        Learn patterns from the provided training text.</span></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="co">        The training process:</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Preprocess the text into tokens</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Create sequences of 'order' words</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Record what word follows each sequence</span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Track valid starting sequences</span></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a><span class="co">            text: Training text to learn patterns from</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> <span class="va">self</span>._preprocess_text(text)</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(words) <span class="op">&lt;</span> <span class="va">self</span>.order <span class="op">+</span> <span class="dv">1</span>:</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f"Training text too short. Need at least </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>order <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss"> words."</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build the transition chain</span></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(words) <span class="op">-</span> <span class="va">self</span>.order):</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a tuple of 'order' words as the key</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>            key <span class="op">=</span> <span class="bu">tuple</span>(words[i:i <span class="op">+</span> <span class="va">self</span>.order])</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a>            <span class="co"># The next word is the value</span></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a>            next_word <span class="op">=</span> words[i <span class="op">+</span> <span class="va">self</span>.order]</span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.chain[key].append(next_word)</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Track sentence starts (after periods)</span></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> words[i <span class="op">-</span> <span class="dv">1</span>].endswith(<span class="st">'.'</span>):</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.starts.append(key)</span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If no starts found, use all keys as potential starts</span></span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.starts:</span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.starts <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.chain.keys())</span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, prompt: <span class="bu">str</span> <span class="op">=</span> <span class="st">""</span>, max_words: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a><span class="co">        Generate text based on learned patterns.</span></span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a><span class="co">        The generation process:</span></span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a><span class="co">        1. Start with the prompt or a random starting sequence</span></span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a><span class="co">        2. Look up what words can follow the current sequence</span></span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a><span class="co">        3. Randomly choose one of those words</span></span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a><span class="co">        4. Shift the window and repeat</span></span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a><span class="co">            prompt: Optional starting text (may be modified to fit chain)</span></span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a><span class="co">            max_words: Maximum number of words to generate</span></span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a><span class="co">            Generated text string</span></span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.chain:</span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="st">"Error: Model not trained. Please train with text first."</span></span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Try to use the prompt, or start randomly</span></span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> prompt:</span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a>            prompt_words <span class="op">=</span> <span class="va">self</span>._preprocess_text(prompt)</span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(prompt_words) <span class="op">&gt;=</span> <span class="va">self</span>.order:</span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>                current <span class="op">=</span> <span class="bu">tuple</span>(prompt_words[<span class="op">-</span><span class="va">self</span>.order:])</span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> current <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.chain:</span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a>                    current <span class="op">=</span> random.choice(<span class="va">self</span>.starts)</span>
<span id="cb9-142"><a href="#cb9-142" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb9-143"><a href="#cb9-143" aria-hidden="true" tabindex="-1"></a>                current <span class="op">=</span> random.choice(<span class="va">self</span>.starts)</span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> random.choice(<span class="va">self</span>.starts)</span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate words</span></span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> <span class="bu">list</span>(current)</span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_words <span class="op">-</span> <span class="va">self</span>.order):</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> current <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.chain:</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a>            next_word <span class="op">=</span> random.choice(<span class="va">self</span>.chain[current])</span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>            result.append(next_word)</span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> <span class="bu">tuple</span>(result[<span class="op">-</span><span class="va">self</span>.order:])</span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">' '</span>.join(result)</span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocabulary_size(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the number of unique word sequences learned."""</span></span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.chain)</span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> total_transitions(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the total number of transitions learned."""</span></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(<span class="bu">len</span>(words) <span class="cf">for</span> words <span class="kw">in</span> <span class="va">self</span>.chain.values())</span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMGenerator:</span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a><span class="co">    A wrapper for OpenAI's GPT models demonstrating modern LLM capabilities.</span></span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a><span class="co">    This represents the current state-of-the-art in text generation (2020s).</span></span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a><span class="co">    Unlike Markov chains, LLMs:</span></span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a><span class="co">    - Understand context and meaning</span></span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a><span class="co">    - Can follow complex instructions</span></span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a><span class="co">    - Generate coherent long-form text</span></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a><span class="co">    - Answer questions accurately</span></span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a><span class="co">    - Adapt to different styles and formats</span></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a><span class="co">        client: OpenAI API client</span></span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a><span class="co">        model: Which GPT model to use</span></span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature: Controls randomness (0=deterministic, 1=creative)</span></span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb9-190"><a href="#cb9-190" aria-hidden="true" tabindex="-1"></a>        model: <span class="bu">str</span> <span class="op">=</span> <span class="st">"gpt-3.5-turbo"</span>,</span>
<span id="cb9-191"><a href="#cb9-191" aria-hidden="true" tabindex="-1"></a>        temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize the LLM generator.</span></span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a><span class="co">            model: OpenAI model to use (e.g., "gpt-3.5-turbo", "gpt-4")</span></span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a><span class="co">            temperature: Sampling temperature (0.0 to 2.0)</span></span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.temperature <span class="op">=</span> temperature</span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the OpenAI client</span></span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a>        api_key <span class="op">=</span> os.getenv(<span class="st">"OPENAI_API_KEY"</span>)</span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> api_key:</span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a>                <span class="st">"OpenAI API key not found. "</span></span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Please set OPENAI_API_KEY in your .env file."</span></span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.client <span class="op">=</span> OpenAI(api_key<span class="op">=</span>api_key)</span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(</span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a>        prompt: <span class="bu">str</span>, </span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>        system_prompt: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a>        max_tokens: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a><span class="co">        Generate text using the OpenAI API.</span></span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-221"><a href="#cb9-221" aria-hidden="true" tabindex="-1"></a><span class="co">        This method demonstrates the chat completion API, which is the</span></span>
<span id="cb9-222"><a href="#cb9-222" aria-hidden="true" tabindex="-1"></a><span class="co">        standard interface for modern LLMs. Key concepts:</span></span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a><span class="co">        - System prompt: Sets the AI's behavior and context</span></span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a><span class="co">        - User prompt: The actual query or request</span></span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a><span class="co">        - Temperature: Controls creativity vs. consistency</span></span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a><span class="co">        - Max tokens: Limits response length</span></span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-228"><a href="#cb9-228" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-229"><a href="#cb9-229" aria-hidden="true" tabindex="-1"></a><span class="co">            prompt: The user's input/question</span></span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a><span class="co">            system_prompt: Optional context/instructions for the AI</span></span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a><span class="co">            max_tokens: Maximum response length</span></span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a><span class="co">            Generated text from the LLM</span></span>
<span id="cb9-235"><a href="#cb9-235" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-236"><a href="#cb9-236" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> system_prompt:</span>
<span id="cb9-237"><a href="#cb9-237" aria-hidden="true" tabindex="-1"></a>            system_prompt <span class="op">=</span> (</span>
<span id="cb9-238"><a href="#cb9-238" aria-hidden="true" tabindex="-1"></a>                <span class="st">"You are a helpful research assistant. "</span></span>
<span id="cb9-239"><a href="#cb9-239" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Provide clear, accurate, and well-organized responses."</span></span>
<span id="cb9-240"><a href="#cb9-240" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb9-241"><a href="#cb9-241" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-242"><a href="#cb9-242" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb9-243"><a href="#cb9-243" aria-hidden="true" tabindex="-1"></a>            response <span class="op">=</span> <span class="va">self</span>.client.chat.completions.create(</span>
<span id="cb9-244"><a href="#cb9-244" aria-hidden="true" tabindex="-1"></a>                model<span class="op">=</span><span class="va">self</span>.model,</span>
<span id="cb9-245"><a href="#cb9-245" aria-hidden="true" tabindex="-1"></a>                messages<span class="op">=</span>[</span>
<span id="cb9-246"><a href="#cb9-246" aria-hidden="true" tabindex="-1"></a>                    {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: system_prompt},</span>
<span id="cb9-247"><a href="#cb9-247" aria-hidden="true" tabindex="-1"></a>                    {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb9-248"><a href="#cb9-248" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb9-249"><a href="#cb9-249" aria-hidden="true" tabindex="-1"></a>                temperature<span class="op">=</span><span class="va">self</span>.temperature,</span>
<span id="cb9-250"><a href="#cb9-250" aria-hidden="true" tabindex="-1"></a>                max_tokens<span class="op">=</span>max_tokens</span>
<span id="cb9-251"><a href="#cb9-251" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb9-252"><a href="#cb9-252" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> response.choices[<span class="dv">0</span>].message.content</span>
<span id="cb9-253"><a href="#cb9-253" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-254"><a href="#cb9-254" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb9-255"><a href="#cb9-255" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="ss">f"Error generating response: </span><span class="sc">{</span><span class="bu">str</span>(e)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb9-256"><a href="#cb9-256" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-257"><a href="#cb9-257" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_with_context(</span>
<span id="cb9-258"><a href="#cb9-258" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb9-259"><a href="#cb9-259" aria-hidden="true" tabindex="-1"></a>        prompt: <span class="bu">str</span>,</span>
<span id="cb9-260"><a href="#cb9-260" aria-hidden="true" tabindex="-1"></a>        context: <span class="bu">str</span>,</span>
<span id="cb9-261"><a href="#cb9-261" aria-hidden="true" tabindex="-1"></a>        max_tokens: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb9-262"><a href="#cb9-262" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb9-263"><a href="#cb9-263" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-264"><a href="#cb9-264" aria-hidden="true" tabindex="-1"></a><span class="co">        Generate a response with additional context.</span></span>
<span id="cb9-265"><a href="#cb9-265" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-266"><a href="#cb9-266" aria-hidden="true" tabindex="-1"></a><span class="co">        This demonstrates how LLMs can use provided information to give</span></span>
<span id="cb9-267"><a href="#cb9-267" aria-hidden="true" tabindex="-1"></a><span class="co">        more accurate, grounded responses, a preview of RAG (Retrieval</span></span>
<span id="cb9-268"><a href="#cb9-268" aria-hidden="true" tabindex="-1"></a><span class="co">        Augmented Generation) that we'll explore in Chapter 5.</span></span>
<span id="cb9-269"><a href="#cb9-269" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-270"><a href="#cb9-270" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-271"><a href="#cb9-271" aria-hidden="true" tabindex="-1"></a><span class="co">            prompt: The user's question</span></span>
<span id="cb9-272"><a href="#cb9-272" aria-hidden="true" tabindex="-1"></a><span class="co">            context: Additional information to consider</span></span>
<span id="cb9-273"><a href="#cb9-273" aria-hidden="true" tabindex="-1"></a><span class="co">            max_tokens: Maximum response length</span></span>
<span id="cb9-274"><a href="#cb9-274" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb9-275"><a href="#cb9-275" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-276"><a href="#cb9-276" aria-hidden="true" tabindex="-1"></a><span class="co">            Context-aware generated response</span></span>
<span id="cb9-277"><a href="#cb9-277" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-278"><a href="#cb9-278" aria-hidden="true" tabindex="-1"></a>        system_prompt <span class="op">=</span> (</span>
<span id="cb9-279"><a href="#cb9-279" aria-hidden="true" tabindex="-1"></a>            <span class="st">"You are a helpful research assistant. "</span></span>
<span id="cb9-280"><a href="#cb9-280" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Use the provided context to answer the user's question. "</span></span>
<span id="cb9-281"><a href="#cb9-281" aria-hidden="true" tabindex="-1"></a>            <span class="st">"If the context doesn't contain relevant information, "</span></span>
<span id="cb9-282"><a href="#cb9-282" aria-hidden="true" tabindex="-1"></a>            <span class="st">"say so and provide your best general knowledge answer."</span></span>
<span id="cb9-283"><a href="#cb9-283" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-284"><a href="#cb9-284" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-285"><a href="#cb9-285" aria-hidden="true" tabindex="-1"></a>        full_prompt <span class="op">=</span> <span class="ss">f"Context:</span><span class="ch">\n</span><span class="sc">{</span>context<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">Question: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb9-286"><a href="#cb9-286" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-287"><a href="#cb9-287" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.generate(</span>
<span id="cb9-288"><a href="#cb9-288" aria-hidden="true" tabindex="-1"></a>            prompt<span class="op">=</span>full_prompt,</span>
<span id="cb9-289"><a href="#cb9-289" aria-hidden="true" tabindex="-1"></a>            system_prompt<span class="op">=</span>system_prompt,</span>
<span id="cb9-290"><a href="#cb9-290" aria-hidden="true" tabindex="-1"></a>            max_tokens<span class="op">=</span>max_tokens</span>
<span id="cb9-291"><a href="#cb9-291" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="understanding-the-code" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-code">2.2 Understanding the Code</h3>
<p><strong>The Markov Chain Generator:</strong></p>
<ul>
<li>Learns by recording which words follow which sequences</li>
<li>Uses probability to choose the next word</li>
<li>Has no understanding of meaning, just patterns</li>
<li>Represents 1990s-era statistical NLP</li>
</ul>
<p><strong>The LLM Generator:</strong></p>
<ul>
<li>Uses OpenAI’s API to access GPT models</li>
<li>Understands context, instructions, and meaning</li>
<li>Can adapt to different tasks and styles</li>
<li>Represents current state-of-the-art</li>
</ul>
<hr>
</section>
</section>
<section id="step-3-creating-the-application-interface" class="level2">
<h2 class="anchored" data-anchor-id="step-3-creating-the-application-interface">Step 3: Creating the Application Interface</h2>
<p>Now let’s build a beautiful, educational interface using Streamlit.</p>
<section id="the-complete-application" class="level3">
<h3 class="anchored" data-anchor-id="the-complete-application">3.1 The Complete Application</h3>
<p>Create <code>app.py</code> with the following content:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">AI Research Assistant - Chapter 1</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">Foundations of Generative AI</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">This Streamlit application demonstrates the evolution of text generation</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">by comparing:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">1. Markov Chain generation (statistical, 1990s)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">2. Large Language Model generation (neural, 2020s)</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">Features:</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">- Side-by-side comparison of generation approaches</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">- Educational explanations of how each method works</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">- Interactive controls for experimentation</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">- Sample training data for the Markov model</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">Run with: streamlit run app.py</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> streamlit <span class="im">as</span> st</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> generators <span class="im">import</span> MarkovChainGenerator, LLMGenerator</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Page Configuration</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>st.set_page_config(</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    page_title<span class="op">=</span><span class="st">"AI Research Assistant"</span>,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    page_icon<span class="op">=</span><span class="st">"🔬"</span>,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    layout<span class="op">=</span><span class="st">"wide"</span>,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    initial_sidebar_state<span class="op">=</span><span class="st">"expanded"</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample Training Data</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co"># This text trains our Markov chain model</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="co"># It covers research and AI topics so the model can generate</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co"># somewhat relevant text for research queries</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>SAMPLE_TRAINING_TEXT <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="st">Artificial intelligence research has made remarkable progress in recent years.</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="st">Machine learning models can now understand and generate human language with </span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="st">impressive accuracy. Natural language processing enables computers to read, </span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="st">understand, and generate text that sounds human-written.</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="st">Research in deep learning has led to breakthroughs in many fields. Neural </span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="st">networks can recognize images, translate languages, and even write code.</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="st">The transformer architecture revolutionized how we process sequential data.</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="st">Scientific research requires careful methodology and rigorous analysis.</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="st">Researchers collect data, form hypotheses, and test their theories through</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="st">experiments. The scientific method ensures that findings are reliable and</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="st">reproducible.</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="st">Large language models learn from vast amounts of text data. They understand</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="st">context, grammar, and even subtle nuances in language. These models can</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="st">answer questions, summarize documents, and assist with writing tasks.</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="st">The future of AI research holds exciting possibilities. Researchers are</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="st">working on making AI systems more efficient, more capable, and more aligned</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="st">with human values. Understanding how these systems work is crucial for</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a><span class="st">developing them responsibly.</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Session State</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initialize_session_state():</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Initialize all session state variables."""</span></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'markov_model'</span> <span class="kw">not</span> <span class="kw">in</span> st.session_state:</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>        st.session_state.markov_model <span class="op">=</span> MarkovChainGenerator(order<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>        st.session_state.markov_model.train(SAMPLE_TRAINING_TEXT)</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'llm_model'</span> <span class="kw">not</span> <span class="kw">in</span> st.session_state:</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>            st.session_state.llm_model <span class="op">=</span> LLMGenerator()</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>            st.session_state.llm_available <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">ValueError</span> <span class="im">as</span> e:</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>            st.session_state.llm_model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>            st.session_state.llm_available <span class="op">=</span> <span class="va">False</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>            st.session_state.llm_error <span class="op">=</span> <span class="bu">str</span>(e)</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>initialize_session_state()</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Sidebar</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> st.sidebar:</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>    st.title(<span class="st">"🔬 Research Assistant"</span>)</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>    st.subheader(<span class="st">"📚 About This Project"</span>)</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"""</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="st">    This is **Chapter 1** of your AI journey!</span></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a><span class="st">    You're exploring two fundamentally different </span></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="st">    approaches to text generation:</span></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a><span class="st">    **🎲 Markov Chains** (1990s)</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="st">    - Statistical patterns</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a><span class="st">    - No understanding</span></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a><span class="st">    - Fast but limited</span></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a><span class="st">    **🧠 Large Language Models** (2020s)</span></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="st">    - Deep understanding</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="st">    - Context awareness</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="st">    - Powerful but complex</span></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>)</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>    st.subheader(<span class="st">"⚙️ Settings"</span>)</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>    generation_method <span class="op">=</span> st.radio(</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Generation Method:"</span>,</span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"Compare Both"</span>, <span class="st">"Markov Only"</span>, <span class="st">"LLM Only"</span>],</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>        <span class="bu">help</span><span class="op">=</span><span class="st">"Choose which generation method(s) to use"</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>    max_words <span class="op">=</span> st.slider(</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Max Words (Markov):"</span>,</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>        min_value<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>        max_value<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span><span class="dv">75</span>,</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>        <span class="bu">help</span><span class="op">=</span><span class="st">"Maximum words for Markov generation"</span></span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>    st.subheader(<span class="st">"💡 Sample Questions"</span>)</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>    sample_questions <span class="op">=</span> [</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>        <span class="st">"What is artificial intelligence?"</span>,</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>        <span class="st">"How do neural networks learn?"</span>,</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Explain the scientific method"</span>,</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>        <span class="st">"What are transformers in AI?"</span>,</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a>        <span class="st">"How does machine learning work?"</span></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> q <span class="kw">in</span> sample_questions:</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> st.button(q, key<span class="op">=</span><span class="ss">f"sample_</span><span class="sc">{</span>q<span class="sc">}</span><span class="ss">"</span>):</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>            st.session_state.current_query <span class="op">=</span> q</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a><span class="co"># Main Content</span></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>st.title(<span class="st">"🔬 AI Research Assistant"</span>)</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>st.markdown(<span class="st">"### Comparing Statistical and Neural Text Generation"</span>)</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>st.markdown(<span class="st">"""</span></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a><span class="st">Welcome to your AI Research Assistant! This tool demonstrates the dramatic </span></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a><span class="st">evolution in text generation technology by comparing **Markov chains** </span></span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a><span class="st">(a 1990s statistical approach) with **modern Large Language Models**.</span></span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a><span class="st">Enter a research question below to see how each approach responds.</span></span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a><span class="co"># Input Section</span></span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>col1, col2 <span class="op">=</span> st.columns([<span class="dv">3</span>, <span class="dv">1</span>])</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> col1:</span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if we have a sample question to use</span></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>    default_query <span class="op">=</span> st.session_state.get(<span class="st">'current_query'</span>, <span class="st">''</span>)</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>    query <span class="op">=</span> st.text_area(</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Enter your research question:"</span>,</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>        value<span class="op">=</span>default_query,</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a>        placeholder<span class="op">=</span><span class="st">"e.g., What is artificial intelligence and how does it work?"</span></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> col2:</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"&lt;br&gt;"</span>, unsafe_allow_html<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>    generate_button <span class="op">=</span> st.button(<span class="st">"🚀 Generate Responses"</span>, <span class="bu">type</span><span class="op">=</span><span class="st">"primary"</span>, use_container_width<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> st.button(<span class="st">"🔄 Clear"</span>, use_container_width<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>        st.session_state.current_query <span class="op">=</span> <span class="st">''</span></span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>        st.rerun()</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a><span class="co"># Generation Section</span></span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> generate_button <span class="kw">and</span> query:</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>    st.subheader(<span class="st">"📊 Results Comparison"</span>)</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create columns based on selected method</span></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> generation_method <span class="op">==</span> <span class="st">"Compare Both"</span>:</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a>        col_markov, col_llm <span class="op">=</span> st.columns(<span class="dv">2</span>)</span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> generation_method <span class="op">==</span> <span class="st">"Markov Only"</span>:</span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>        col_markov <span class="op">=</span> st.container()</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a>        col_llm <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>        col_markov <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a>        col_llm <span class="op">=</span> st.container()</span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Markov Chain Generation</span></span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col_markov:</span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> col_markov:</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a>            st.markdown(<span class="st">"### 🎲 Markov Chain Response"</span>)</span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a>            st.caption(<span class="st">"Statistical text generation (1990s approach)"</span>)</span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> st.spinner(<span class="st">"Generating with Markov chain..."</span>):</span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>                markov_response <span class="op">=</span> st.session_state.markov_model.generate(</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>                    prompt<span class="op">=</span>query,</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>                    max_words<span class="op">=</span>max_words</span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a>            st.markdown(<span class="ss">f"**Generated Text:**"</span>)</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a>            st.info(markov_response)</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> st.expander(<span class="st">"ℹ️ How Markov Chains Work"</span>):</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>                st.markdown(<span class="st">"""</span></span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a><span class="st">                **Markov chains** generate text by:</span></span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a><span class="st">                </span></span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a><span class="st">                1. **Learning patterns** from training text</span></span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a><span class="st">                2. **Looking at the last N words** (order of the chain)</span></span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a><span class="st">                3. **Randomly selecting** a word that followed </span></span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a><span class="st">                   that sequence in training</span></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a><span class="st">                </span></span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a><span class="st">                **Limitations:**</span></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a><span class="st">                - No understanding of meaning</span></span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a><span class="st">                - Can only reproduce seen patterns</span></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a><span class="st">                - Often produces incoherent text</span></span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="st">                - Cannot answer questions accurately</span></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a><span class="st">                </span></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a><span class="st">                **Statistics for this model:**</span></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a><span class="st">                """</span>)</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a>                st.write(<span class="ss">f"- Vocabulary size: </span><span class="sc">{</span>st<span class="sc">.</span>session_state<span class="sc">.</span>markov_model<span class="sc">.</span>vocabulary_size<span class="sc">}</span><span class="ss"> sequences"</span>)</span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a>                st.write(<span class="ss">f"- Total transitions: </span><span class="sc">{</span>st<span class="sc">.</span>session_state<span class="sc">.</span>markov_model<span class="sc">.</span>total_transitions<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM Generation</span></span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col_llm:</span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> col_llm:</span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>            st.markdown(<span class="st">"### 🧠 LLM Response"</span>)</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a>            st.caption(<span class="st">"Neural network generation (2020s approach)"</span>)</span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> st.session_state.llm_available:</span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> st.spinner(<span class="st">"Generating with GPT..."</span>):</span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a>                    llm_response <span class="op">=</span> st.session_state.llm_model.generate(prompt<span class="op">=</span>query)</span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a>                st.markdown(<span class="ss">f"**Generated Text:**"</span>)</span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>                st.success(llm_response)</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> st.expander(<span class="st">"ℹ️ How LLMs Work"</span>):</span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>                    st.markdown(<span class="st">"""</span></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a><span class="st">                    **Large Language Models** generate text by:</span></span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a><span class="st">                    </span></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a><span class="st">                    1. **Understanding context** through attention mechanisms</span></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a><span class="st">                    2. **Processing your entire prompt** simultaneously</span></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a><span class="st">                    3. **Predicting tokens** based on learned patterns</span></span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a><span class="st">                       from billions of text examples</span></span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a><span class="st">                    </span></span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a><span class="st">                    **Capabilities:**</span></span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a><span class="st">                    - Deep understanding of meaning</span></span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a><span class="st">                    - Can follow complex instructions</span></span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a><span class="st">                    - Generates coherent, relevant responses</span></span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a><span class="st">                    - Adapts to different styles and tasks</span></span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a><span class="st">                    </span></span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a><span class="st">                    **This model:** GPT-3.5-turbo</span></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a><span class="st">                    """</span>)</span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a>                st.error(<span class="st">"⚠️ LLM not available"</span>)</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a>                st.warning(<span class="ss">f"Error: </span><span class="sc">{</span>st<span class="sc">.</span>session_state<span class="sc">.</span>get(<span class="st">'llm_error'</span>, <span class="st">'Unknown error'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a>                st.info(<span class="st">"To enable LLM generation, add your OpenAI API key to the .env file"</span>)</span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a><span class="co"># Educational Footer</span></span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a>st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> st.expander(<span class="st">"📖 Learning More: The Evolution of Text Generation"</span>):</span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>    st.markdown(<span class="st">"""</span></span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a><span class="st">    ## From Statistics to Neural Networks</span></span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a><span class="st">    The difference you see between these two approaches represents </span></span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a><span class="st">    **decades of AI research progress**.</span></span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a><span class="st">    ### The Statistical Era (1980s-2010s)</span></span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a><span class="st">    Early text generation relied on counting patterns:</span></span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a><span class="st">    - **N-gram models** counted word sequences</span></span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Hidden Markov Models** added state transitions</span></span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Statistical Machine Translation** used phrase tables</span></span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="st">    These approaches were limited because they had no real </span></span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a><span class="st">    "understanding", just pattern matching.</span></span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a><span class="st">    ### The Neural Revolution (2010s-Present)</span></span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a><span class="st">    Neural networks changed everything:</span></span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Word embeddings** captured meaning in numbers</span></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Recurrent networks** added memory</span></span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Transformers** enabled parallel processing</span></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a><span class="st">    - **Large Language Models** achieved human-like text</span></span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a><span class="st">    ### What's Next?</span></span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a><span class="st">    In the upcoming chapters, you'll learn to:</span></span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="st">    - Fine-tune models for specific tasks</span></span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="st">    - Build retrieval-augmented generation (RAG) systems</span></span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a><span class="st">    - Deploy AI applications to production</span></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a><span class="st">    - Evaluate and improve model outputs</span></span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a><span class="st">    """</span>)</span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a>st.markdown(<span class="st">"---"</span>)</span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a>st.caption(<span class="st">"Chapter 1: Foundations of Generative AI | AI Research Assistant v1.0"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="interface-components-explained" class="level3">
<h3 class="anchored" data-anchor-id="interface-components-explained">3.2 Interface Components Explained</h3>
<p>The interface includes:</p>
<ul>
<li><strong>① Title and Introduction</strong>: Sets context for the application</li>
<li><strong>② Sidebar Navigation</strong>: Settings and sample questions</li>
<li><strong>③ Input Area</strong>: Text area for user queries</li>
<li><strong>④ Generation Controls</strong>: Buttons for generating and clearing</li>
<li><strong>⑤ Method Selection</strong>: Radio buttons to choose generation approach</li>
<li><strong>⑥ Results Display</strong>: Side-by-side comparison of outputs</li>
<li><strong>⑦ Educational Content</strong>: Expandable explanations of how each method works</li>
<li><strong>⑧ Model Statistics</strong>: Information about the Markov chain model</li>
<li><strong>⑨ Error Handling</strong>: Graceful handling of missing API keys</li>
<li><strong>⑩ Sample Questions</strong>: Quick-start buttons for common queries</li>
<li><strong>⑪ Word Limit Control</strong>: Slider for Markov generation length</li>
<li><strong>⑫ Visual Indicators</strong>: Icons and colors for different sections</li>
<li><strong>⑬ Loading States</strong>: Spinners during generation</li>
<li><strong>⑭ Rich Results Display</strong>: Shows responses, metrics, and explanations</li>
<li><strong>⑮ Continuous Learning</strong>: Side-by-side educational content</li>
<li><strong>⑯ Development Tools</strong>: Easy reset for testing and development</li>
</ul>
<hr>
</section>
</section>
<section id="step-4-running-your-research-assistant" class="level2">
<h2 class="anchored" data-anchor-id="step-4-running-your-research-assistant">Step 4: Running Your Research Assistant</h2>
<section id="final-project-structure-check" class="level3">
<h3 class="anchored" data-anchor-id="final-project-structure-check">Final Project Structure Check</h3>
<p>Your project should now look like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/chapters/ch01_img12.svg" class="img-fluid figure-img"></p>
<figcaption>Final Project Structure</figcaption>
</figure>
</div>
<p><em>Figure 1.6: Final Project Structure for Chapter 1</em></p>
</section>
<section id="usage-instructions" class="level3">
<h3 class="anchored" data-anchor-id="usage-instructions">Usage Instructions</h3>
<section id="running-the-application" class="level4">
<h4 class="anchored" data-anchor-id="running-the-application">Running the Application</h4>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Activate your virtual environment</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venv/bin/activate  <span class="co"># macOS/Linux</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ex">venv\Scripts\activate</span>     <span class="co"># Windows</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Start the Streamlit application</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">streamlit</span> run app.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="expected-output" class="level4">
<h4 class="anchored" data-anchor-id="expected-output">Expected Output</h4>
<p><strong>Terminal:</strong></p>
<pre><code>You can now view your Streamlit app in your browser.
Local URL: http://localhost:8501
Network URL: http://192.168.1.100:8501</code></pre>
<p><strong>Browser Interface:</strong></p>
<ul>
<li><strong>Left Panel</strong>: Input area and generation controls</li>
<li><strong>Right Panel</strong>: Educational content and learning objectives</li>
<li><strong>Sidebar</strong>: Method selection and sample questions</li>
<li><strong>Results</strong>: Side-by-side comparison of generation approaches</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="key-learning-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="key-learning-outcomes">Key Learning Outcomes</h2>
<section id="technical-skills-developed" class="level3">
<h3 class="anchored" data-anchor-id="technical-skills-developed">Technical Skills Developed</h3>
<ol type="1">
<li><strong>Environment Management</strong>: Virtual environments and dependency handling</li>
<li><strong>API Integration</strong>: Secure credential management and error handling</li>
<li><strong>Statistical AI</strong>: Understanding Markov chain text generation</li>
<li><strong>Modern AI</strong>: Large language model integration and prompt engineering</li>
<li><strong>Web Application Development</strong>: Streamlit interface creation</li>
<li><strong>Comparative Analysis</strong>: Understanding trade-offs between AI approaches</li>
</ol>
</section>
<section id="conceptual-understanding" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h3>
<ol type="1">
<li><strong>Evolution of AI</strong>: From statistical to neural approaches</li>
<li><strong>Text Processing Pipeline</strong>: Normalization, tokenization, filtering</li>
<li><strong>Generation Strategies</strong>: Statistical patterns vs.&nbsp;learned representations</li>
<li><strong>Performance Trade-offs</strong>: Speed vs.&nbsp;quality vs.&nbsp;cost considerations</li>
<li><strong>Production Considerations</strong>: Error handling, security, user experience</li>
</ol>
<p>This foundation provides the building blocks for advanced topics like retrieval-augmented generation (RAG), fine-tuning, and production deployment covered in subsequent chapters.</p>
<hr>
</section>
</section>
<section id="chapter-summary" class="level2">
<h2 class="anchored" data-anchor-id="chapter-summary">Chapter Summary</h2>
<section id="what-weve-accomplished" class="level3">
<h3 class="anchored" data-anchor-id="what-weve-accomplished">What We’ve Accomplished</h3>
<p>In this foundational chapter, you’ve journeyed from understanding the basic concepts of generative AI to building your first AI-powered application. Let’s recap the key milestones:</p>
<p><strong>Conceptual Understanding:</strong></p>
<ul>
<li>Defined generative AI and understood how it differs from discriminative AI</li>
<li>Traced the evolution from ELIZA (1966) to GPT-4 (2023)</li>
<li>Learned how tokenization breaks language into processable units</li>
<li>Understood embeddings as the “geography of meaning”</li>
<li>Explored the transformer architecture and attention mechanisms</li>
<li>Grasped how LLMs learn through next-word prediction at massive scale</li>
</ul>
<p><strong>Practical Skills:</strong></p>
<ul>
<li>Set up a professional Python development environment</li>
<li>Implemented a Markov chain text generator from scratch</li>
<li>Integrated OpenAI’s GPT API for modern text generation</li>
<li>Built an interactive Streamlit web application</li>
<li>Created a side-by-side comparison tool for different AI approaches</li>
</ul>
</section>
<section id="the-journey-ahead" class="level3">
<h3 class="anchored" data-anchor-id="the-journey-ahead">The Journey Ahead</h3>
<p>This research assistant will grow with you throughout this book:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Chapter</th>
<th>What You’ll Add</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>Model selection and comparison tools</td>
</tr>
<tr class="even">
<td>3</td>
<td>Advanced prompt engineering techniques</td>
</tr>
<tr class="odd">
<td>4</td>
<td>Multi-provider API integration</td>
</tr>
<tr class="even">
<td>5</td>
<td>RAG with vector databases</td>
</tr>
<tr class="odd">
<td>6</td>
<td>Fine-tuning for specialized tasks</td>
</tr>
<tr class="even">
<td>7</td>
<td>Multimodal capabilities</td>
</tr>
<tr class="odd">
<td>8</td>
<td>Agent-based interactions</td>
</tr>
<tr class="even">
<td>9</td>
<td>Evaluation and testing frameworks</td>
</tr>
<tr class="odd">
<td>10</td>
<td>Production deployment</td>
</tr>
</tbody>
</table>
</section>
<section id="reflection-points" class="level3">
<h3 class="anchored" data-anchor-id="reflection-points">Reflection Points</h3>
<p>As you move forward, consider these questions:</p>
<ul>
<li>How does understanding the evolution of AI help you appreciate current capabilities?</li>
<li>What limitations did you notice in the Markov chain approach?</li>
<li>How might you improve the research assistant’s responses?</li>
<li>What ethical considerations arose as you built this system?</li>
</ul>
</section>
<section id="congratulations" class="level3">
<h3 class="anchored" data-anchor-id="congratulations">Congratulations!</h3>
<p>You’ve completed the first major milestone in your generative AI journey. You’re no longer just a user of AI systems, you’re becoming a builder of them. This shift in perspective will serve you well as AI continues to transform industries and create new opportunities.</p>
</section>
<section id="the-bigger-picture" class="level3">
<h3 class="anchored" data-anchor-id="the-bigger-picture">The Bigger Picture</h3>
<p>As you continue through this book, remember that you’re learning more than just how to use AI tools, you’re developing the skills to shape how AI gets integrated into research, business, and society. The understanding you’re building of how these systems work, their capabilities and limitations, and the engineering practices needed to deploy them responsibly will be increasingly valuable.</p>
<p>The research assistant you’ve started building today could genuinely become a useful tool for real research work. But more importantly, the skills and understanding you’re developing will enable you to build AI applications that solve real problems and create genuine value.</p>
<p><strong>Welcome to the future of AI development, and congratulations on taking your first significant step as an AI application builder!</strong></p>
<p><em>Ready to dive deeper? In Chapter 2, we’ll explore the fascinating world of different language models and add intelligent model selection to your research assistant. The foundation you’ve built is about to become much more sophisticated.</em></p>
<hr>
</section>
</section>
<section id="end-of-chapter-interactive-content" class="level2">
<h2 class="anchored" data-anchor-id="end-of-chapter-interactive-content">End-of-Chapter Interactive Content</h2>
<section id="assignment-chapter-1-core-project-submission" class="level3">
<h3 class="anchored" data-anchor-id="assignment-chapter-1-core-project-submission">Assignment: Chapter 1 Core Project Submission</h3>
<p><strong>Objective</strong>: Submit your working AI research assistant with comparative text generation capabilities.</p>
<p><strong>Requirements</strong>:</p>
<ol type="1">
<li><p><strong>Code Submission</strong>: Submit your complete project folder including:</p>
<ul>
<li>app.py (main application)</li>
<li>generators.py (generation classes)</li>
<li>requirements.txt (dependencies list)</li>
<li>Screenshots of your running application</li>
</ul></li>
<li><p><strong>Experimentation Report</strong>: Create a brief report (500-750 words) addressing:</p>
<ul>
<li>Compare responses from Markov chain vs LLM for at least 3 different queries</li>
<li>Analyze the strengths and weaknesses of each approach</li>
<li>Describe any challenges you encountered during setup</li>
<li>Suggest one improvement you’d like to add to the current system</li>
</ul></li>
<li><p><strong>Reflection Questions</strong> (Answer in 2-3 sentences each):</p>
<ul>
<li>How does understanding tokenization change your perspective on how AI processes language?</li>
<li>What surprised you most about the difference between Markov chain and LLM outputs?</li>
<li>Based on this chapter, what aspect of generative AI are you most excited to explore further?</li>
</ul></li>
</ol>
<p><strong>Submission Format</strong>:</p>
<ul>
<li>Zip file containing: code folder, report (PDF), reflection answers (PDF or text file)</li>
<li>File naming: Chapter1_[YourLastName]_[YourFirstName].zip</li>
</ul>
<p><strong>Grading Criteria</strong>:</p>
<ul>
<li><strong>Functionality (40%)</strong>: Code runs without errors, both generators work</li>
<li><strong>Analysis (30%)</strong>: Thoughtful comparison and experimentation report</li>
<li><strong>Code Quality (20%)</strong>: Clean, well-commented code following provided structure</li>
<li><strong>Reflection (10%)</strong>: Demonstrates understanding of key concepts</li>
</ul>
<p><strong>Due Date</strong>: [To be specified by instructor]</p>
<hr>
</section>
<section id="discussion-forum-chapter-1---foundations-first-insights" class="level3">
<h3 class="anchored" data-anchor-id="discussion-forum-chapter-1---foundations-first-insights">Discussion Forum: Chapter 1 - Foundations &amp; First Insights</h3>
<p><strong>Welcome to our learning community!</strong></p>
<p>Congratulations on completing Chapter 1 and building your first AI-powered research assistant! You’ve just taken a significant step from being an AI user to becoming an AI builder. This discussion board is where we’ll share insights, learn from each other’s experiences, and build a community of AI practitioners.</p>
<section id="your-introduction-reflection" class="level4">
<h4 class="anchored" data-anchor-id="your-introduction-reflection">Your Introduction &amp; Reflection</h4>
<p>Please introduce yourself to your fellow learners by sharing:</p>
<p><strong>Personal Introduction</strong></p>
<ul>
<li>Your name and background (academic, professional, or personal interest)</li>
<li>What drew you to learn about generative AI and LLMs</li>
<li>Any prior experience with AI, programming, or related fields (don’t worry if this is your first time, we welcome all levels!)</li>
</ul>
<p><strong>Your Biggest “Aha!” Moment</strong></p>
<p>After working through Chapter 1’s concepts and building your research assistant, share <strong>one surprising insight</strong> you gained about generative AI. This could be something that:</p>
<ul>
<li>Changed how you think about AI systems</li>
<li>Surprised you about how AI actually works “under the hood”</li>
<li>Made you realize something new about the evolution from statistical to neural approaches</li>
<li>Emerged from comparing your Markov chain outputs with LLM responses</li>
<li>Challenged a preconception you had about AI technology</li>
</ul>
<p><em>Examples might include: “I was surprised that…” “I never realized that…” “It was fascinating to discover…” “The biggest difference I noticed was…”</em></p>
<p><strong>Your Burning Question</strong></p>
<p>As we embark on this 10-chapter journey together, what’s <strong>one specific question</strong> you’re hoping we’ll answer as we progress? This could be:</p>
<ul>
<li>Something technical you want to understand better</li>
<li>A practical application you’re curious about</li>
<li>An ethical or societal concern about AI</li>
<li>A specific capability you want to learn to build</li>
<li>A challenge you’ve encountered that you hope we’ll address</li>
</ul>
<p><em>Examples: “How do I know which AI model to use for different tasks?” “Can I really build something as sophisticated as ChatGPT?” “How do I ensure my AI applications are unbiased and safe?” “What does it take to deploy AI in a real business environment?”</em></p>
</section>
<section id="discussion-guidelines" class="level4">
<h4 class="anchored" data-anchor-id="discussion-guidelines">Discussion Guidelines</h4>
<p><strong>Engage Meaningfully:</strong></p>
<ul>
<li>Read and respond to at least 2-3 of your classmates’ posts</li>
<li>Ask follow-up questions about their insights or experiences</li>
<li>Share related experiences or observations</li>
<li>Offer encouragement and support, we’re all learning together!</li>
</ul>
<p><strong>Be Curious and Respectful:</strong></p>
<ul>
<li>There are no “dumb” questions here, if you’re wondering about something, others probably are too</li>
<li>Different backgrounds bring different perspectives; embrace this diversity</li>
<li>Share both successes and struggles from building your research assistant</li>
</ul>
<p><strong>Connect and Build:</strong></p>
<ul>
<li>Look for classmates with similar interests or complementary skills</li>
<li>Consider forming study groups or collaboration partnerships</li>
<li>Share resources, articles, or tools you discover along the way</li>
</ul>
</section>
<section id="getting-the-most-from-this-discussion" class="level4">
<h4 class="anchored" data-anchor-id="getting-the-most-from-this-discussion">Getting the Most from This Discussion</h4>
<p>This isn’t just an assignment, it’s the beginning of your network of AI practitioners and learners. The connections you make here could lead to:</p>
<ul>
<li>Study partnerships for challenging concepts</li>
<li>Collaboration opportunities on projects</li>
<li>Professional networking in the AI field</li>
<li>Ongoing learning communities beyond this course</li>
</ul>
<p>Many of the most valuable insights will come not just from the course material, but from seeing how different people approach the same concepts and challenges.</p>
<p><strong>Ready to share?</strong> Jump in with your introduction, insight, and question. We’re excited to get to know you and learn alongside you as we build increasingly sophisticated AI applications together!</p>
<p><strong>Looking forward to your perspectives and to supporting each other through this exciting journey into the world of generative AI!</strong></p>
<p><em>P.S. If you encountered any technical challenges while building your research assistant, feel free to mention them here too, chances are others faced similar issues, and troubleshooting together is a great way to learn!</em></p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../acknowledgments.html" class="pagination-link" aria-label="Acknowledgments">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Acknowledgments</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/02-llms.html" class="pagination-link" aria-label="Chapter 2: The Architecture of Understanding">
        <span class="nav-page-text"><span class="chapter-title">Chapter 2: The Architecture of Understanding</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>